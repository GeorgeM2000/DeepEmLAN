{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GeorgeM2000/DeepEmLAN/blob/main/DeepEmLAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Libraries***"
      ],
      "metadata": {
        "id": "64d9j-gmBWw1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import gc\n",
        "import sys\n",
        "import tensorflow as tf; tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "from math import pow\n",
        "from datetime import datetime\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from time import time\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "7PFt1_A0Baw_"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Global Variables & General Functionality***"
      ],
      "metadata": {
        "id": "BHlh50FCBjuI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "nzOOjI5qBCJP"
      },
      "outputs": [],
      "source": [
        "MAX_LEN = 100\n",
        "MAX_LENS = []\n",
        "neg_table_size = 1000000\n",
        "NEG_SAMPLE_POWER = 0.75\n",
        "batch_size = 64\n",
        "num_epoch = 50\n",
        "embed_size = 200\n",
        "lr = 1e-3\n",
        "\n",
        "tag_size = 7 # Number of categories\n",
        "Beta = 1.0\n",
        "negative_ratio = 1\n",
        "edge_num = 5214 # 5214 # Number of edges # Default: 5429\n",
        "\n",
        "# Importance coefficients\n",
        "rho1 = 0.3\n",
        "rho2 = 0.1\n",
        "rho3 = 0.3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf_ratio = [0.15, 0.45, 0.75]\n",
        "clf_num = 5\n",
        "train_classifier = True"
      ],
      "metadata": {
        "id": "f14uDaQK3o5c"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "node_clf_results_file = 'DeepEmLAN_Node_Clf_Res.txt'\n",
        "link_pred_results_file = 'DeepEmLAN_Link_Pred_Res.txt'\n",
        "log_file = 'DeepEmLAN_Execution_Logs.txt' # Log file to save execution details\n",
        "\n",
        "split_graph_file = 'sgraph15.txt'\n",
        "split_graph_files = ['sgraph15.txt', 'sgraph45.txt', 'sgraph75.txt']\n",
        "test_graph_file = 'tgraph85.txt'\n",
        "test_graph_files = ['tgraph85.txt', 'tgraph55.txt', 'tgraph25.txt']\n",
        "\n",
        "\n",
        "dataset_name = \"cora\"\n",
        "categories_file = 'group-v3.txt'\n",
        "data_text_file = \"data.txt\" # For a single execution\n",
        "graph_file = \"graph.txt\" # For a single execution\n",
        "data_text_files = [\"data.txt\", \"YAKE.txt\", \"YAKE10.txt\"] # Used for multiple executions of the model\n",
        "parent_path = f'/content/Datasets/{dataset_name}'"
      ],
      "metadata": {
        "id": "A2AKcSCx3tyZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for txtf in data_text_files:#['data.txt']:#data_text_files:\n",
        "  max_word_count = 0\n",
        "  min_word_count = float('inf')\n",
        "\n",
        "  with open(f'{parent_path}/{txtf}', 'r') as file:\n",
        "      for line in file:\n",
        "          word_count = len(line.split())\n",
        "\n",
        "          if word_count > max_word_count:\n",
        "              max_word_count = word_count\n",
        "\n",
        "          if word_count < min_word_count:\n",
        "              min_word_count = word_count\n",
        "\n",
        "  MAX_LENS.append(max_word_count+1)\n",
        "  print(f'=== {txtf} ===')\n",
        "  print(\"Max word count:\", max_word_count)\n",
        "  print(\"Min word count:\", min_word_count)\n",
        "  print()\n",
        "\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "NX6wAiiWMt7r",
        "outputId": "eeb0b903-2932-4637-dad1-ff4eee48ecfa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== data.txt ===\n",
            "Max word count: 410\n",
            "Min word count: 30\n",
            "\n",
            "=== YAKE.txt ===\n",
            "Max word count: 15\n",
            "Min word count: 14\n",
            "\n",
            "=== YAKE10.txt ===\n",
            "Max word count: 30\n",
            "Min word count: 27\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENS"
      ],
      "metadata": {
        "id": "i5KRANNNE_o0",
        "outputId": "1d055861-0924-4aa2-eca7-91281a989644",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[411, 16, 31]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = MAX_LENS[-1] # For single execution"
      ],
      "metadata": {
        "id": "7M5mje-xMx00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Open and extract the zip file\n",
        "with zipfile.ZipFile('/content/PartialData.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall(parent_path)\n",
        "\n",
        "print(\"Extraction complete!\")"
      ],
      "metadata": {
        "id": "cwjqMJ96M0I1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vectors_from_file(file_path):\n",
        "  vectors = {}\n",
        "\n",
        "  with open(f'{file_path}', \"r\") as f:\n",
        "      for idx, line in enumerate(f):\n",
        "          vector = list(map(float, line.strip().split()))  # Convert to list of floats\n",
        "          vectors[idx] = vector  # Assign embedding to node idx\n",
        "\n",
        "  return vectors"
      ],
      "metadata": {
        "id": "UpBHSimHTyrC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Dataset***"
      ],
      "metadata": {
        "id": "BQLMk6xzBRgr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class dataSet:\n",
        "\tdef __init__(self, text_path, graph_path, label_dic):\n",
        "\n",
        "\t\ttext_file, graph_file = self.load(text_path, graph_path)\n",
        "\t\tself.edges = self.load_edges(graph_file)\n",
        "\t\tself.label_dic = label_dic # This dictionary contains the label of each node\n",
        "\t\tself.text, self.num_vocab, self.num_nodes = self.load_text(text_file)\n",
        "\t\tself.negative_table = InitNegTable(self.edges) # \"negative_table\" uses only the available edges (depends soley on the graph provided as a parameter)\n",
        "\n",
        "\n",
        "\tdef load(self, text_path, graph_path):\n",
        "\t\ttext_file = open(text_path, 'rb').readlines()\n",
        "\t\tgraph_file = open(graph_path, 'rb').readlines()\n",
        "\t\treturn text_file, graph_file\n",
        "\n",
        "\tdef load_edges(self, graph_file):\n",
        "\t\tedges = [] # \"edges\" will be a 2D list. Each sublist will have two values: [node1, node2]\n",
        "\t\tfor i in graph_file:\n",
        "\t\t\tedges.append(list(map(int, i.split()))) # Original: map(int, i.split()) --> This returns an object that is not subscriptable\n",
        "\t\treturn edges\n",
        "\n",
        "\t# Use when the text file has 1s and 0s in each line representing presence or absence of a word\n",
        "\t''' def load_text(self, text_file):\n",
        "\t\ttext_one = [0] * MAX_LEN # Create a zeros vector with \"MAX_LEN\" length\n",
        "\t\ttext = []\n",
        "\t\tfor te in text_file:\n",
        "\t\t\ti = 0\n",
        "\t\t\tj = 0\n",
        "\t\t\ttte = te.split()[1:]\n",
        "\t\t\tfor t in tte:\n",
        "\t\t\t\tif t == '1.0' or t == '1' or t == '1.':\n",
        "\t\t\t\t\ttext_one[j] = i+1\n",
        "\t\t\t\t\tj = j+1\n",
        "\t\t\t\ti = i+1\n",
        "\t\t\ttext.append(text_one)\n",
        "\t\t\ttext_one = [0] * MAX_LEN\n",
        "\n",
        "\t\ttext = np.array(text)\n",
        "\t\tnum_vocab = len(text_file[0].split()) + 1\n",
        "\t\tself.num_nodes = len(text)\n",
        "\t\treturn text, num_vocab, self.num_nodes '''\n",
        "\n",
        "\n",
        "\tdef load_text(self, text_file):\n",
        "\t\t#text_data = [line.strip() for line in text_file]\n",
        "\t\ttext_data = [line.decode('utf-8').strip() for line in text_file] # Decode each line from bytes to string using 'utf-8'\n",
        "\n",
        "\t\ttokenizer = Tokenizer(oov_token=None) # Default: Tokenizer()\n",
        "\t\ttokenizer.fit_on_texts(text_data)\n",
        "\n",
        "\t\ttext = tokenizer.texts_to_sequences(text_data)\n",
        "\t\ttext = pad_sequences(text, maxlen=MAX_LEN, padding=\"post\", truncating='post') # Default: pad_sequences(text, maxlen=MAX_LEN, padding=\"post\")\n",
        "\n",
        "\t\tnum_vocab = len(tokenizer.word_index) + 1  # +1 for padding token\n",
        "\t\tnum_nodes = len(text)\n",
        "\t\treturn text, num_vocab, num_nodes\n",
        "\n",
        "\n",
        "\tdef negative_sample(self, edges):\n",
        "\t\tnode1, node2 = list(zip(*edges))[0:2] # Original: zip(*edges)[0:2] --> Leads to error\n",
        "\t\tneg_sample_edges = []\n",
        "\t\tfunc = lambda: self.negative_table[random.randint(0, neg_table_size-1)]\n",
        "\t\tfor i in range(len(edges)):\n",
        "\t\t\tneg_node = func()\n",
        "\t\t\twhile self.label_dic[str(node1[i])] == self.label_dic[str(neg_node)] or self.label_dic[str(node2[i])] == \\\n",
        "\t\t\t\t\tself.label_dic[str(neg_node)]:\n",
        "\t\t\t\tneg_node = func()\n",
        "\t\t\tneg_sample_edges.append(neg_node)\n",
        "\n",
        "\t\treturn neg_sample_edges\n",
        "\n",
        "\tdef generate_batches(self, mode=None):\n",
        "\t\tr_one_hot = np.zeros(tag_size, dtype=float)\n",
        "\t\tbr_one_hot = np.ones(tag_size, dtype=float)\n",
        "\t\tnum_batch = len(self.edges) // batch_size # Original: len(self.edges) / batch_size --> The result is a float which cannot be used for list slicing\n",
        "\t\tedge_l = self.edges\n",
        "\t\tfor i in range(0, edge_num):\n",
        "\t\t\tr_one_hot[int(self.label_dic[str(edge_l[i][0])])] = 1.0\n",
        "\t\t\tr_one_hot[int(self.label_dic[str(edge_l[i][1])])] = 1.0\n",
        "\t\t\tbr_one_hot[int(self.label_dic[str(edge_l[i][0])])] = Beta\n",
        "\t\t\tbr_one_hot[int(self.label_dic[str(edge_l[i][1])])] = Beta\n",
        "\t\t\tedge_l[i].append(r_one_hot)\n",
        "\t\t\tedge_l[i].append(br_one_hot)\n",
        "\t\t\tr_one_hot = np.zeros(tag_size, dtype=float)\n",
        "\t\t\tbr_one_hot = np.ones(tag_size, dtype=float)\n",
        "\t\tif mode == 'add':\n",
        "\t\t\tnum_batch += 1\n",
        "\t\t\tedge_l.extend(edge_l[:(batch_size - len(self.edges) // batch_size)])\n",
        "\t\tif mode != 'add':\n",
        "\t\t\trandom.shuffle(edge_l)\n",
        "\n",
        "\n",
        "\t\tsample_edges = edge_l[:num_batch * batch_size]\n",
        "\t\tsample_neg = self.negative_sample(sample_edges)\n",
        "\n",
        "\n",
        "\n",
        "\t\tshuffle_edges = []\n",
        "\t\tshuffle_relation = []\n",
        "\t\tshuffle_brelation = []\n",
        "\t\t# shuffle_edgetags = []\n",
        "\t\tfor edge in sample_edges:\n",
        "\t\t\tshuffle_edges.append([edge[0], edge[1]])\n",
        "\t\t\tshuffle_relation.append(edge[2])\n",
        "\t\t\tshuffle_brelation.append(edge[3])\n",
        "\t\tbatches_edges = []\n",
        "\t\tbatches_re = []\n",
        "\t\tbatches_neg = []\n",
        "\t\tbatches_br = []\n",
        "\t\tfor j in range(num_batch):\n",
        "\t\t\tbatches_edges.append(shuffle_edges[j * batch_size:(j + 1) * batch_size])\n",
        "\t\t\tbatches_re.append(np.array(shuffle_relation[j * batch_size:(j + 1) * batch_size]))\n",
        "\t\t\tbatches_neg.append(sample_neg[j * batch_size:(j + 1) * batch_size])\n",
        "\t\t\tbatches_br.append(np.array(shuffle_brelation[j * batch_size:(j + 1) * batch_size]))\n",
        "\n",
        "\t\treturn batches_edges, batches_re, batches_neg, batches_br"
      ],
      "metadata": {
        "id": "UeQ4pb6qBKDF"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***DeepEmLAN***"
      ],
      "metadata": {
        "id": "Mt0xoLN7BuZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model:\n",
        "    def __init__(self, data, order):\n",
        "        tf.compat.v1.reset_default_graph() # With every new model all the variables used in a previous model will be reset\n",
        "        self.cur_epoch = 0\n",
        "        self.vocab_size = data.num_vocab\n",
        "        self.num_nodes = data.num_nodes\n",
        "        self.order = order\n",
        "        self.data = data\n",
        "        self.sess = tf.compat.v1.Session()\n",
        "        self.n_hidden_1 = 64\n",
        "        self.n_hidden_2 = 32\n",
        "        self.output_size = tag_size\n",
        "\n",
        "        cur_seed = random.getrandbits(32)\n",
        "        initializer = tf.keras.initializers.GlorotNormal(seed=cur_seed)\n",
        "        with tf.compat.v1.variable_scope(\"model\", reuse=None, initializer=initializer):\n",
        "            self.build_graph()\n",
        "        self.sess.run(tf.compat.v1.global_variables_initializer())\n",
        "\n",
        "    def close_session(self):\n",
        "        self.sess.close()\n",
        "\n",
        "    def build_graph(self):\n",
        "            # '''hyperparameter'''\n",
        "        with tf.name_scope('read_inputs') as scope:\n",
        "            self.Text_a = tf.compat.v1.placeholder(tf.int32, [batch_size, MAX_LEN], name='Ta' + str(self.order))\n",
        "            self.Text_b = tf.compat.v1.placeholder(tf.int32, [batch_size, MAX_LEN], name='Tb' + str(self.order))\n",
        "            self.Text_pos = tf.compat.v1.placeholder(tf.int32, [batch_size, MAX_LEN], name='Tpos'+ str(self.order))\n",
        "            self.Text_neg = tf.compat.v1.placeholder(tf.int32, [batch_size, MAX_LEN], name='Tneg'+ str(self.order))\n",
        "            self.Node_a = tf.compat.v1.placeholder(tf.int32, [batch_size], name='n1' + str(self.order))\n",
        "            self.Node_b = tf.compat.v1.placeholder(tf.int32, [batch_size], name='n2' + str(self.order))\n",
        "            self.Node_pa = tf.compat.v1.placeholder(tf.int32, [batch_size], name='n4' + str(self.order))\n",
        "            self.Node_na = tf.compat.v1.placeholder(tf.int32, [batch_size], name='n5' + str(self.order))\n",
        "            self.relation = tf.compat.v1.placeholder(tf.float32, [batch_size, tag_size], name='r' + str(self.order))\n",
        "            self.brelation = tf.compat.v1.placeholder(tf.float32, [batch_size, tag_size], name='br' + str(self.order))\n",
        "\n",
        "        with tf.name_scope('initialize_para') as scope:\n",
        "            self.weights = {\n",
        "                'encoder_h1'+ str(self.order): tf.Variable(tf.compat.v1.random_normal([embed_size // 2, self.n_hidden_1])),\n",
        "                'encoder_h2'+ str(self.order): tf.Variable(tf.compat.v1.random_normal([self.n_hidden_1, self.n_hidden_2])),\n",
        "                'encoder_h3'+ str(self.order): tf.Variable(tf.compat.v1.random_normal([self.n_hidden_2, self.output_size]))\n",
        "            }\n",
        "            cur_seed = random.getrandbits(32)\n",
        "            self.biases = {\n",
        "                'encoder_b1'+ str(self.order): tf.Variable(tf.compat.v1.random_normal([self.n_hidden_1])),\n",
        "                'encoder_b2'+ str(self.order): tf.Variable(tf.compat.v1.random_normal([self.n_hidden_2])),\n",
        "                'encoder_b3'+ str(self.order): tf.Variable(tf.compat.v1.random_normal([self.output_size]))\n",
        "            }\n",
        "\n",
        "        with tf.name_scope('initialize_embedding') as scope:\n",
        "            cur_seed = random.getrandbits(32)\n",
        "            self.context_embed = tf.compat.v1.get_variable(name=\"context_embeddings\"+ str(self.order),\n",
        "                                                 shape=[self.num_nodes, embed_size // 2],\n",
        "                                                 initializer=tf.keras.initializers.GlorotNormal(seed=cur_seed))\n",
        "            self.text_embed = tf.compat.v1.get_variable(name=\"text_embeddings\"+ str(self.order),\n",
        "                                              shape=[self.vocab_size, embed_size // 2],\n",
        "                                              initializer=tf.keras.initializers.GlorotNormal(seed=cur_seed))\n",
        "\n",
        "            self.node_embed = tf.compat.v1.get_variable(name=\"embeddings\" + str(self.order),\n",
        "                                              shape=[self.num_nodes, embed_size // 2],\n",
        "                                              initializer=tf.keras.initializers.GlorotNormal(seed=cur_seed))\n",
        "\n",
        "        with tf.name_scope('lookup_embeddings') as scope:\n",
        "            self.TA = tf.nn.embedding_lookup(self.text_embed, self.Text_a)\n",
        "            self.T_A = tf.expand_dims(self.TA, -1)\n",
        "\n",
        "            self.TB = tf.nn.embedding_lookup(self.text_embed, self.Text_b)\n",
        "            self.T_B = tf.expand_dims(self.TB, -1)\n",
        "\n",
        "            self.Tpos = tf.nn.embedding_lookup(self.text_embed, self.Text_pos)\n",
        "            self.T_POS = tf.expand_dims(self.Tpos, -1)\n",
        "\n",
        "            self.TNEG = tf.nn.embedding_lookup(self.text_embed, self.Text_neg)\n",
        "            self.T_NEG = tf.expand_dims(self.TNEG, -1)\n",
        "\n",
        "            self.N_A = tf.nn.l2_normalize(tf.nn.embedding_lookup(self.node_embed, self.Node_a), 1)\n",
        "            self.N_B = tf.nn.l2_normalize(tf.nn.embedding_lookup(self.node_embed, self.Node_b), 1)\n",
        "            self.N_POS = tf.nn.embedding_lookup(self.node_embed, self.Node_pa)\n",
        "            self.N_NEG = tf.nn.embedding_lookup(self.node_embed, self.Node_na)\n",
        "            self.N_NEG_list = tf.split(self.N_NEG, negative_ratio, 1)\n",
        "\n",
        "            self.pos_nb_context = tf.nn.l2_normalize(\n",
        "                tf.nn.embedding_lookup(self.context_embed, tf.cast(self.Node_b, tf.int32)), 1)\n",
        "            self.pos_ab = tf.nn.l2_normalize(\n",
        "                tf.nn.embedding_lookup(self.node_embed, tf.cast(self.Node_pa, tf.int32)), 1)\n",
        "            self.neg_ab = tf.nn.l2_normalize(\n",
        "                tf.nn.embedding_lookup(self.node_embed, tf.cast(self.Node_na, tf.int32)), 1)\n",
        "            self.neg_ab_context = tf.nn.l2_normalize(\n",
        "                tf.nn.embedding_lookup(self.context_embed, tf.cast(self.Node_na, tf.int32)), 1)\n",
        "\n",
        "        self.R_AB, self.l_predict = self.conv()\n",
        "        self.loss = self.compute_loss(self.order)\n",
        "        optimizer = tf.compat.v1.train.AdamOptimizer(lr)\n",
        "        self.train_op = optimizer.minimize(self.loss)\n",
        "\n",
        "    def train_one_epoch(self):\n",
        "        loss_epoch = 0\n",
        "        batches_edges, batches_re, batches_neg, batches_br = self.data.generate_batches()\n",
        "        num_batch = len(batches_edges)\n",
        "        for i in range(num_batch):\n",
        "            node1, node2 = zip(*batches_edges[i])\n",
        "            node_list4 = batches_neg[i]\n",
        "            batch_r = batches_re[i]\n",
        "            batch_br = batches_br[i]\n",
        "            node_list3 = node2\n",
        "            node1, node2, node_list3, node_list4 = np.array(node1), np.array(node2), \\\n",
        "                                                          np.array(node_list3), np.array(node_list4)\n",
        "            node_list3 = np.transpose(node_list3)\n",
        "            text1, text2 = self.data.text[node1], self.data.text[node2]\n",
        "            text_pos = []\n",
        "            text_neg = []\n",
        "            for npp in node_list3:\n",
        "                text_pos.append(self.data.text[npp])\n",
        "            for nn in node_list4:\n",
        "                text_neg.append(self.data.text[nn])\n",
        "\n",
        "            feed_dict = {\n",
        "                self.Text_a: text1,\n",
        "                self.Text_b: text2,\n",
        "                self.Text_pos: text_pos,\n",
        "                self.Text_neg: text_neg,\n",
        "                self.Node_a: node1,\n",
        "                self.Node_b: node2,\n",
        "                self.Node_pa: node_list3,\n",
        "                self.Node_na: node_list4,\n",
        "                self.relation: batch_r,\n",
        "                self.brelation: batch_br\n",
        "\n",
        "            }\n",
        "\n",
        "            # run the graph\n",
        "            _, loss_batch = self.sess.run([self.train_op, self.loss], feed_dict=feed_dict)\n",
        "            # print loss_batch\n",
        "            loss_epoch += loss_batch\n",
        "        #print str('order' + str(self.order) + ':'), self.cur_epoch, ' loss: ', loss_epoch\n",
        "        self.cur_epoch += 1\n",
        "\n",
        "    def encoder(self, x):\n",
        "        layer_1 = tf.nn.tanh(tf.add(tf.matmul(tf.reshape(x,[batch_size, embed_size // 2]), self.weights['encoder_h1'+ str(self.order)]),\n",
        "                                       self.biases['encoder_b1'+ str(self.order)]))\n",
        "        layer_2 = tf.nn.tanh(tf.add(tf.matmul(layer_1, self.weights['encoder_h2'+ str(self.order)]),\n",
        "                                       self.biases['encoder_b2'+ str(self.order)]))\n",
        "        layer_3 = tf.reshape(tf.nn.tanh(tf.add(tf.matmul(layer_2, self.weights['encoder_h3'+ str(self.order)]),\n",
        "                                       self.biases['encoder_b3'+ str(self.order)])),[batch_size,1,self.output_size])\n",
        "        return layer_3\n",
        "\n",
        "    ''' def conv(self):\n",
        "\n",
        "        TA_norm = tf.sqrt(tf.reduce_sum(self.TA*self.TA, 1))\n",
        "        TB_norm = tf.sqrt(tf.reduce_sum(self.TB*self.TB, 1))\n",
        "        TA_TB = tf.reduce_sum(self.TA*self.TB, 1)\n",
        "        # self.TN = tf.reshape(tf.split(self.TNEG, config.negative_ratio, 1)[0],\n",
        "        #            [config.batch_size, int(config.embed_size / 2), int(config.embed_size / 2)])\n",
        "        cosinAB = tf.divide(TA_TB, TA_norm*TB_norm+1e-8)\n",
        "\n",
        "        cosin1 = tf.expand_dims(cosinAB, -1)\n",
        "\n",
        "\n",
        "        self.u_A = tf.reshape(tf.matmul(self.TA, cosin1),   [batch_size, embed_size // 2])\n",
        "        self.u_B = tf.reshape(tf.matmul(self.TB, cosin1),   [batch_size, embed_size // 2])\n",
        "        self.u_P = tf.reshape(tf.matmul(self.Tpos, cosin1), [batch_size, embed_size // 2])\n",
        "        self.u_N = tf.reshape(tf.matmul(self.TNEG, cosin1), [batch_size, embed_size // 2])\n",
        "\n",
        "\n",
        "        R_AB = self.u_A + self.u_B\n",
        "        self.R_AB = tf.reshape(R_AB, [batch_size, embed_size // 2])\n",
        "\n",
        "        l_predict = self.encoder(tf.reshape(self.R_AB, [batch_size, 1, embed_size // 2]))\n",
        "        return R_AB, l_predict '''\n",
        "\n",
        "\n",
        "    # Used for debugging\n",
        "    def conv(self):\n",
        "        # Compute norms\n",
        "        TA_norm = tf.sqrt(tf.reduce_sum(self.TA * self.TA, axis=1))\n",
        "        TB_norm = tf.sqrt(tf.reduce_sum(self.TB * self.TB, axis=1))\n",
        "        TA_TB = tf.reduce_sum(self.TA * self.TB, axis=1)\n",
        "\n",
        "        ''' print(\"TA shape:\", self.TA.shape)\n",
        "        print(\"TB shape:\", self.TB.shape)\n",
        "        print(\"TA_norm shape:\", TA_norm.shape)\n",
        "        print(\"TB_norm shape:\", TB_norm.shape)\n",
        "        print(\"TA_TB shape:\", TA_TB.shape) '''\n",
        "\n",
        "        # Compute cosine similarity\n",
        "        cosinAB = tf.divide(TA_TB, TA_norm * TB_norm + 1e-8)\n",
        "        #print(\"cosinAB shape:\", cosinAB.shape)\n",
        "\n",
        "        # Expand dimensions\n",
        "        cosin1 = tf.expand_dims(cosinAB, -1)\n",
        "        #print(\"cosin1 shape:\", cosin1.shape)\n",
        "\n",
        "        # Compute transformed representations\n",
        "        self.u_A = tf.reshape(tf.broadcast_to(tf.squeeze(tf.matmul(self.TA, cosin1), axis=-1)[..., tf.newaxis], [batch_size, MAX_LEN, embed_size // 2]), [batch_size * MAX_LEN, embed_size // 2])\n",
        "        self.u_B = tf.reshape(tf.broadcast_to(tf.squeeze(tf.matmul(self.TB, cosin1), axis=-1)[..., tf.newaxis], [batch_size, MAX_LEN, embed_size // 2]), [batch_size * MAX_LEN, embed_size // 2])\n",
        "        self.u_P = tf.reshape(tf.broadcast_to(tf.squeeze(tf.matmul(self.Tpos, cosin1), axis=-1)[..., tf.newaxis], [batch_size, MAX_LEN, embed_size // 2]), [batch_size * MAX_LEN, embed_size // 2])\n",
        "        self.u_N = tf.reshape(tf.broadcast_to(tf.squeeze(tf.matmul(self.TNEG, cosin1), axis=-1)[..., tf.newaxis], [batch_size, MAX_LEN, embed_size // 2]), [batch_size * MAX_LEN, embed_size // 2])\n",
        "\n",
        "        ''' print(\"u_A shape:\", self.u_A.shape)\n",
        "        print(\"u_B shape:\", self.u_B.shape)\n",
        "        print(\"u_P shape:\", self.u_P.shape)\n",
        "        print(\"u_N shape:\", self.u_N.shape) '''\n",
        "\n",
        "        # Compute final representation\n",
        "        R_AB = self.u_A + self.u_B\n",
        "        self.R_AB = tf.reshape(tf.broadcast_to(R_AB,  [batch_size, MAX_LEN, embed_size // 2]), [batch_size * MAX_LEN, embed_size // 2])\n",
        "        #print(\"R_AB shape:\", R_AB.shape)\n",
        "        #print(\"self.R_AB shape:\", self.R_AB.shape)\n",
        "\n",
        "        # Pass through encoder\n",
        "        l_predict = self.encoder(tf.reshape(tf.broadcast_to(self.R_AB, [batch_size, 1, embed_size // 2]) [batch_size, 1, embed_size // 2]))\n",
        "        #print(\"l_predict shape:\", l_predict.shape)\n",
        "\n",
        "\n",
        "        return R_AB, l_predict\n",
        "\n",
        "    def compute_loss(self,order):\n",
        "\n",
        "        rho1 = 0.3\n",
        "        rho2 = 0.1\n",
        "        rho3 = 0.3\n",
        "        p1 = tf.reduce_sum(tf.multiply(self.u_A, self.u_B), 1)\n",
        "        p1 = tf.math.log(tf.nn.sigmoid(p1) + 0.001)\n",
        "        p2 = tf.reduce_sum(tf.multiply(self.N_A, self.N_B), 1)\n",
        "        p2 = tf.math.log(tf.nn.sigmoid(p2) + 0.001)\n",
        "\n",
        "        p3 = tf.reduce_sum(tf.multiply(self.N_A, self.u_A), 1)\n",
        "        p3 = tf.math.log(tf.nn.sigmoid(p3) + 0.001)\n",
        "        p4 = tf.math.reduce_sum(tf.multiply(self.N_B, self.u_B), 1)\n",
        "        p4 = tf.math.log(tf.nn.sigmoid(p4) + 0.001)\n",
        "\n",
        "\n",
        "        for i in range(0, 1):\n",
        "        # for i in range(0, config.negative_ratio):\n",
        "            u_P1 = tf.reshape(tf.split(self.u_P, negative_ratio, 1)[i], [batch_size, embed_size // 2])\n",
        "            u_N1 = tf.reshape(tf.split(self.u_N, negative_ratio, 1)[i],\n",
        "                              [batch_size, embed_size // 2])\n",
        "            p5 = tf.reduce_sum(tf.multiply(self.u_A, u_P1), 1)\n",
        "            p5 = tf.math.log(tf.nn.sigmoid(p5) + 0.001)\n",
        "            p6 = tf.reduce_sum(tf.multiply(self.u_B, u_P1), 1)\n",
        "            p6 = tf.math.log(tf.nn.sigmoid(p6) + 0.001)\n",
        "            p7 = tf.reduce_sum(tf.multiply(self.u_A, u_N1), 1)\n",
        "            p7 = tf.math.log(tf.nn.sigmoid(-p7) + 0.001)\n",
        "            p8 = tf.reduce_sum(tf.multiply(self.u_B, u_N1), 1)\n",
        "            p8 = tf.math.log(tf.nn.sigmoid(-p8) + 0.001)\n",
        "            p9 = tf.reduce_sum(tf.multiply(self.N_A, self.N_NEG), 1)\n",
        "            p9 = tf.math.log(tf.nn.sigmoid(-p9) + 0.001)\n",
        "            p10 = tf.reduce_sum(tf.multiply(self.N_B, self.N_NEG), 1)\n",
        "            p10 = tf.math.log(tf.nn.sigmoid(-p10) + 0.001)\n",
        "\n",
        "        p11 = tf.reduce_sum(tf.multiply(self.l_predict, self.relation))\n",
        "        p11 = tf.math.log(tf.nn.sigmoid(p11) + 0.001)\n",
        "\n",
        "        p_all = rho1*(p1 + p2 + p5 + p6 + p7 + p8 + p9 + p10) + rho2*(p3 + p4) + rho3 * p11\n",
        "\n",
        "        temp_loss = -tf.reduce_sum(p_all+p11)\n",
        "        self.sample_sum1 = tf.reduce_sum(tf.exp(tf.multiply(self.pos_ab, self.neg_ab)),\n",
        "                                         axis=1)\n",
        "        self.first_loss = tf.reduce_mean(-tf.reduce_sum(tf.multiply(self.N_A, self.N_B), axis=1) +\n",
        "                                         tf.math.log(self.sample_sum1))\n",
        "        self.sample_sum2 = tf.reduce_sum(\n",
        "            tf.exp(tf.multiply(self.pos_ab, self.neg_ab_context)), axis=1)\n",
        "        self.second_loss = tf.reduce_mean(-tf.reduce_sum(tf.multiply(self.N_A, self.pos_nb_context), axis=1) +\n",
        "                                          tf.math.log(self.sample_sum2))\n",
        "        loss = temp_loss + self.first_loss + self.second_loss\n",
        "\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def get_embedding(self):\n",
        "        vectors = {}\n",
        "        available_vectors = {} # Nodes that have an embedding: 1 --- Nodes that don't: 0\n",
        "\n",
        "        zero_list = []\n",
        "        for i in range(0, embed_size):\n",
        "            zero_list.append(0)\n",
        "        zero_list = np.array(zero_list)\n",
        "\n",
        "\n",
        "        embed = [[] for _ in range(self.data.num_nodes)]\n",
        "\n",
        "        batches_edges, batches_re, batches_neg, batches_br = self.data.generate_batches(mode='add')\n",
        "        num_batch = len(batches_edges)\n",
        "        for i in range(num_batch):\n",
        "            node1, node2 = zip(*batches_edges[i])\n",
        "            node_list4 = batches_neg[i]\n",
        "            batch_r = batches_re[i]\n",
        "            batch_br = batches_br[i]\n",
        "\n",
        "            node_list3 = node2\n",
        "            node1, node2, node_list3, node_list4 = np.array(node1), np.array(node2), \\\n",
        "                                                                 np.array(node_list3), np.array(\n",
        "                node_list4)\n",
        "            node_list3 = np.transpose(node_list3)\n",
        "            text1, text2 = self.data.text[node1], self.data.text[node2]\n",
        "            text_pos = []\n",
        "            text_neg = []\n",
        "            for npp in node_list3:\n",
        "                text_pos.append(self.data.text[npp])\n",
        "            for nn in node_list4:\n",
        "                text_neg.append(self.data.text[nn])\n",
        "\n",
        "\n",
        "            feed_dict = {\n",
        "                self.Text_a: text1,\n",
        "                self.Text_b: text2,\n",
        "                self.Text_pos: text_pos,\n",
        "                self.Text_neg: text_neg,\n",
        "                self.Node_a: node1,\n",
        "                self.Node_b: node2,\n",
        "                self.Node_pa: node_list3,\n",
        "                self.Node_na: node_list4,\n",
        "                self.relation: batch_r,\n",
        "                self.brelation: batch_br\n",
        "\n",
        "            }\n",
        "            uA, uB, rAB, NA, NB = self.sess.run([self.u_A, self.u_B, self.R_AB, self.N_A, self.N_B], feed_dict=feed_dict)\n",
        "\n",
        "            for i in range(batch_size):\n",
        "                embed[node1[i]].append(list(NA[i])+list(rAB[i]))  #\n",
        "                embed[node2[i]].append(list(NB[i])+list(rAB[i]))  #\n",
        "\n",
        "        for i in range(self.data.num_nodes):\n",
        "            if embed[i]: # If an embedding exists for node i\n",
        "                tmp=np.sum(embed[i],axis=0)/len(embed[i])\n",
        "                vectors[i]=tmp\n",
        "                available_vectors[i] = 1\n",
        "                #file.write(' '.join(map(str,tmp))+'\\n')\n",
        "            else:\n",
        "                vectors[i]=zero_list\n",
        "                available_vectors[i] = 0\n",
        "\n",
        "        return vectors, available_vectors"
      ],
      "metadata": {
        "id": "_gZqZFbABvuX"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Negative table***"
      ],
      "metadata": {
        "id": "ABKadvVjB7St"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def InitNegTable(edges):\n",
        "\ta_list,b_list=zip(*edges)\n",
        "\ta_list=list(a_list)\n",
        "\tb_list=list(b_list)\n",
        "\tnode=a_list\n",
        "\tnode.extend(b_list)\n",
        "\n",
        "\tnode_degree={}\n",
        "\tfor i in node:\n",
        "\t\tif i in node_degree:\n",
        "\t\t\tnode_degree[i]+=1\n",
        "\t\telse:\n",
        "\t\t\tnode_degree[i]=1\n",
        "\tsum_degree=0\n",
        "\tfor i in node_degree.values():\n",
        "\t\tsum_degree+=pow(i,0.75)\n",
        "\n",
        "\tpor=0\n",
        "\tcur_sum=0\n",
        "\tvid=-1\n",
        "\tneg_table=[]\n",
        "\tdegree_list=list(node_degree.values())\n",
        "\tnode_id=list(node_degree.keys())\n",
        "\tfor i in range(neg_table_size):\n",
        "\t\tif(((i+1)/float(neg_table_size))>por):\n",
        "\t\t\tcur_sum+=pow(degree_list[vid+1],NEG_SAMPLE_POWER)\n",
        "\t\t\tpor=cur_sum/sum_degree\n",
        "\t\t\tvid+=1\n",
        "\t\tneg_table.append(node_id[vid])\n",
        "\treturn neg_table"
      ],
      "metadata": {
        "id": "12O1kWaqB-0x"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Classify***"
      ],
      "metadata": {
        "id": "-y0bRdobCaxz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TopKRanker(OneVsRestClassifier):\n",
        "    def predict(self, X, top_k_list):\n",
        "        probs = np.asarray(super(TopKRanker, self).predict_proba(X))\n",
        "        all_labels = []\n",
        "        for i, k in enumerate(top_k_list):\n",
        "            probs_ = probs[i, :]\n",
        "            labels = self.classes_[probs_.argsort()[-k:]].tolist()\n",
        "            probs_[:] = 0\n",
        "            probs_[labels] = 1\n",
        "            all_labels.append(probs_)\n",
        "        return np.asarray(all_labels)\n",
        "\n",
        "\n",
        "class Classifier(object):\n",
        "\n",
        "    def __init__(self, vectors, clf):\n",
        "        self.embeddings = vectors\n",
        "        self.clf = TopKRanker(clf)\n",
        "        self.binarizer = MultiLabelBinarizer(sparse_output=True)\n",
        "\n",
        "    def train(self, X, Y, Y_all):\n",
        "        self.binarizer.fit(Y_all)\n",
        "        # X_train = [self.embeddings[x] for x in X]\n",
        "        X_train = [self.embeddings[int(x)] for x in X] # For each node in X, take its embedding\n",
        "        Y = self.binarizer.transform(Y)\n",
        "        self.clf.fit(X_train, Y)\n",
        "\n",
        "    def evaluate(self, X, Y):\n",
        "        top_k_list = [len(l) for l in Y] # For each label in Y, take its size (multi-label)\n",
        "        Y_ = self.predict(X, top_k_list)\n",
        "        Y = self.binarizer.transform(Y)\n",
        "        averages = [\"micro\", \"macro\"]\n",
        "        results = {}\n",
        "        for average in averages:\n",
        "            results[average] = f1_score(Y, Y_, average=average)\n",
        "        return results\n",
        "\n",
        "    def predict(self, X, top_k_list):\n",
        "        X_ = np.asarray([self.embeddings[int(x)] for x in X])\n",
        "        Y = self.clf.predict(X_, top_k_list=top_k_list)\n",
        "        return Y\n",
        "\n",
        "    def split_train_evaluate(self, X, Y, train_precent, seed=0):\n",
        "        state = np.random.get_state()\n",
        "        training_size = int(train_precent * len(X)) # Set the ratio based on the size of X\n",
        "        np.random.seed(seed)\n",
        "        shuffle_indices = np.random.permutation(np.arange(len(X))) # Shuffle the indices of X (X contains all nodes)\n",
        "\n",
        "        # Access the values of X and Y based on the shuffled indices\n",
        "\n",
        "        # X_train and Y_train will have \"training_size\" number of values of X and Y\n",
        "        X_train = [X[shuffle_indices[i]] for i in range(training_size)]\n",
        "        Y_train = [Y[shuffle_indices[i]] for i in range(training_size)]\n",
        "\n",
        "        # X_test and Y_test will have \"len(X) - training_size\" number of values of X and Y\n",
        "        X_test = [X[shuffle_indices[i]] for i in range(training_size, len(X))]\n",
        "        Y_test = [Y[shuffle_indices[i]] for i in range(training_size, len(X))]\n",
        "\n",
        "        self.train(X_train, Y_train, Y) # Y has the labels of all nodes\n",
        "        np.random.set_state(state)\n",
        "        return self.evaluate(X_test, Y_test)\n",
        "\n",
        "\n",
        "\n",
        "def load_embeddings(filename):\n",
        "    fin = open(filename, 'r')\n",
        "    node_num, size = [int(x) for x in fin.readline().strip().split()]\n",
        "    vectors = {}\n",
        "    while 1:\n",
        "        l = fin.readline()\n",
        "        if l == '':\n",
        "            break\n",
        "        vec = l.strip().split(' ')\n",
        "        assert len(vec) == size + 1\n",
        "        vectors[vec[0]] = [float(x) for x in vec[1:]]\n",
        "    fin.close()\n",
        "    assert len(vectors) == node_num\n",
        "    return vectors\n",
        "\n",
        "def read_node_label(filename):\n",
        "    fin = open(filename, 'r')\n",
        "    X = []\n",
        "    Y = []\n",
        "    XY_dic = {}\n",
        "    X_Y_dic = {}\n",
        "    while 1:\n",
        "        l = fin.readline()\n",
        "        if l == '':\n",
        "            break\n",
        "        # vec = l.strip().split('\\t')\n",
        "        vec = l.strip().split(' ')\n",
        "        X.append(vec[0])\n",
        "        Y.append(vec[1:])\n",
        "        X_Y_dic[str(vec[0])] = str(vec[1:][0])\n",
        "        XY_dic.setdefault(str(vec[1:][0]), []).append(str(vec[0]))\n",
        "    fin.close()\n",
        "    return X, Y, XY_dic, X_Y_dic"
      ],
      "metadata": {
        "id": "irxGxTo8CcpG"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Train***"
      ],
      "metadata": {
        "id": "-zOfZReFCOI1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code that needs to be executed only once"
      ],
      "metadata": {
        "id": "dfXKB5YC3cCS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_key(dict, value):\n",
        "    return [k for k, v in dict.items() if v == value]\n",
        "\n",
        "\n",
        "# Store the label of each abstract\n",
        "label_dic = {}\n",
        "with open(f'{parent_path}/{categories_file}', 'r') as f:\n",
        "  labels = f.readlines()\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "The \"node_tag\" text file has to be in the following format: node_id node_label1 node_label2 ...\n",
        "\"\"\"\n",
        "for la in labels:\n",
        "  label_dic[la.split()[0]] = la.split()[1:][0] # la.split()[0] = the node id ----- la.split()[1:][0] = The label of that node. If a node has many labels, take the first\n",
        "\n",
        "zero_list = []\n",
        "\n",
        "# Place \"embed_size * 2\" (400) zeros in \"zero_list\"\n",
        "for i in range(0, embed_size):\n",
        "  zero_list.append(0)"
      ],
      "metadata": {
        "id": "VELZ1oY_3k-T"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Single Execution***"
      ],
      "metadata": {
        "id": "OXY6tbddHIaz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save embeddings with a unique name\n",
        "#embed_file = f\"{parent_path}/Results/DeepEmLAN/embed_link_pred_{graph_file.split('.')[0]}_{data_text_file.split('.')[0]}.txt\"\n",
        "embed_file = f\"{parent_path}/Results/DeepEmLAN/embed_node_clf_{graph_file.split('.')[0]}_{data_text_file.split('.')[0]}.txt\""
      ],
      "metadata": {
        "id": "qtL36FG8WNNQ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the edge list. Store the unique nodes in the list \"nodes\"\n",
        "with open(f'{parent_path}/{graph_file}', 'r') as f:\n",
        "  eedges = f.readlines()\n",
        "\n",
        "edge_list = []\n",
        "nodes = [] # \"nodes\" will contain all the unique nodes of the graph\n",
        "for ee in eedges:\n",
        "  edge_list.append(list(ee.split()))\n",
        "for ll in edge_list:\n",
        "  for ed in ll:\n",
        "    if ed not in nodes:\n",
        "      nodes.append(ed)\n",
        "    else:\n",
        "      continue\n",
        "\n",
        "\n",
        "# Create the dataset. It seems that dataSet() takes the full graph. Perhaps because it is meant for node classification and not for link prediction.\n",
        "# In link prediction, other NE methods split the graph and the dataSet() takes only a portion of the graph\n",
        "data = dataSet(f'{parent_path}/{data_text_file}', f'{parent_path}/{graph_file}', label_dic)\n",
        "\n",
        "\n",
        "# Train the model for \"num_epoch\" epochs\n",
        "model = Model(data, 1)\n",
        "\n",
        "\n",
        "start_time = datetime.now()\n",
        "for i in range(num_epoch):\n",
        "  model.train_one_epoch()\n",
        "\n",
        "vectors, available_vectors = model.get_embedding() # Get the node embeddings\n",
        "\n",
        "\n",
        "\n",
        "# Store all the node ids with the same label as node \"ii\".\n",
        "node_nei_list = {}\n",
        "one_node_edges = []\n",
        "for ii in nodes: # For each node \"ii\"\n",
        "  for ed in edge_list: # For each edge \"ed\"\n",
        "    if ii in ed: # If node \"ii\" is in \"ed\". This takes the one-hop neighborhood of \"ii\"\n",
        "      if label_dic[ii] == label_dic[ed[0]] and ii not in one_node_edges: # If the label of \"ii\" is the same as the label of \"ed[0]\" and \"ii\" not in \"one_node_edges\"\n",
        "        one_node_edges.append(ed[0])\n",
        "      if label_dic[ii] == label_dic[ed[1]] and ii not in one_node_edges:\n",
        "        one_node_edges.append(ed[1])\n",
        "    else:\n",
        "      pass\n",
        "  node_nei_list[ii] = one_node_edges # Node \"ii\" has the same label with nodes in \"one_node_edges\"\n",
        "  one_node_edges = []\n",
        "\n",
        "# Change the vector representation of each node\n",
        "new_vector = {}\n",
        "one_node_new_vec = []\n",
        "for ve in vectors.keys(): # For each node that has an embedding in \"vectors\"\n",
        "  if str(ve) in node_nei_list:\n",
        "    for nnl in node_nei_list[str(ve)]: # Take the nodes with the same label as node \"ve\"\n",
        "      one_node_new_vec.append(vectors[int(nnl)]) # Append the embeddings of all nodes with the same label as \"ve\"\n",
        "    #one_node_new_vec = np.array(one_node_new_vec).sum(axis=0)/len(node_nei_list[str(ve)]) # Take the mean\n",
        "\n",
        "\n",
        "    if one_node_new_vec: # Check if one_node_new_vec has elements before calculating the mean to avoid ZeroDivisionError\n",
        "      one_node_new_vec = np.array(one_node_new_vec).sum(axis=0)/len(node_nei_list[str(ve)]) # Take the mean\n",
        "    else: # Assign zero_list if the node has no neighbors with the same label\n",
        "      one_node_new_vec = zero_list\n",
        "\n",
        "    # The new vector for node \"ve\" will be the mean of all the embeddings of the one-hop neighbors with the same label as \"ve\"\n",
        "    new_vector[ve] = one_node_new_vec\n",
        "    one_node_new_vec = []\n",
        "  else:\n",
        "    new_vector[ve] = zero_list\n",
        "\n",
        "end_time = datetime.now()\n",
        "print(f'Time: {((end_time - start_time).total_seconds()) / 60.0}')\n",
        "model.close_session()\n",
        "\n",
        "\n",
        "# For link prediction\n",
        "''' with open(embed_file, 'wb') as f:\n",
        "  for node_id, node_vec in new_vector.items():\n",
        "    if available_vectors[node_id] == 1:\n",
        "        f.write((' '.join(map(str, node_vec)) + '\\n').encode())\n",
        "    else:\n",
        "        f.write('\\n'.encode()) '''\n",
        "\n",
        "# For node classification\n",
        "with open(embed_file, 'wb') as f:\n",
        "  for node_id, node_vec in new_vector.items():\n",
        "    f.write((' '.join(map(str, node_vec)) + '\\n').encode())\n",
        "\n",
        "# Log completion\n",
        "with open(f'{parent_path}/Results/DeepEmLAN/{log_file}', 'a') as f:\n",
        "    f.write(f'Embeddings saved to: {embed_file}\\n')\n",
        "\n",
        "gc.collect()\n"
      ],
      "metadata": {
        "id": "I1Wa1lNLCP5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Link Prediction*** (For both single and multiple executions)"
      ],
      "metadata": {
        "id": "Qjl3aVXJuZMh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_files = [[f'{parent_path}/Results/DeepEmLAN/embed_link_pred_graph_data.txt']]\n",
        "\n",
        "# Initialize a log file to store the AUC results\n",
        "with open(f'{parent_path}/Results/DeepEmLAN/{link_pred_results_file}', \"a\") as f:\n",
        "    f.write(\"Embed File\\tAUC Value\\n\")\n",
        "\n",
        "for tgfi, tgf in enumerate(test_graph_files):\n",
        "  for ef in embed_files[tgfi]:\n",
        "      node2vec = {}\n",
        "\n",
        "      # Load the embeddings from the current embed file\n",
        "      with open(ef, 'rb') as f:\n",
        "          for i, j in enumerate(f):\n",
        "              if j.decode().strip():\n",
        "                  node2vec[i] = list(map(float, j.strip().decode().split(' ')))\n",
        "\n",
        "      # Load the edges from the test graph file\n",
        "      with open(f'{parent_path}/{tgf}', 'rb') as f:\n",
        "          edges = [list(map(int, i.strip().decode().split())) for i in f]\n",
        "\n",
        "      nodes = list(set([i for j in edges for i in j])) # All the unique nodes in \"edges\"\n",
        "\n",
        "      # Calculate AUC\n",
        "      a = 0\n",
        "      b = 0\n",
        "      for i, j in edges:\n",
        "          if i in node2vec.keys() and j in node2vec.keys():\n",
        "              dot1 = np.dot(node2vec[i], node2vec[j])\n",
        "              random_node = random.sample(nodes, 1)[0]\n",
        "              while random_node == j or random_node not in node2vec.keys():\n",
        "                  random_node = random.sample(nodes, 1)[0]\n",
        "              dot2 = np.dot(node2vec[i], node2vec[random_node])\n",
        "              if dot1 > dot2:\n",
        "                  a += 1\n",
        "              elif dot1 == dot2:\n",
        "                  a += 0.5\n",
        "              b += 1\n",
        "\n",
        "      auc_value = float(a) / b if b > 0 else 0\n",
        "      print(f\"AUC value for {ef.split('/')[-1]}: {auc_value}\")\n",
        "\n",
        "      # Log the result\n",
        "      with open(f'{parent_path}/Results/DeepEmLAN/{link_pred_results_file}', \"a\") as f:\n",
        "          f.write(f\"{ef}\\t{tgf}\\t{auc_value}\\n\")\n",
        "\n",
        "      gc.collect()"
      ],
      "metadata": {
        "id": "o9vCWo-WGvKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Node Classification*** (For both single and multiple executions)"
      ],
      "metadata": {
        "id": "LivQHT84uc-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_files = [f'{parent_path}/Results/DeepEmLAN/embed_node_clf_graph_data.txt',\n",
        "               f'{parent_path}/Results/DeepEmLAN/embed_node_clf_graph_YAKE.txt',\n",
        "               f'{parent_path}/Results/DeepEmLAN/embed_node_clf_graph_YAKE10.txt',\n",
        "               f'{parent_path}/Results/DeepEmLAN/embed_node_clf_graph_PositionRank.txt',\n",
        "               f'{parent_path}/Results/DeepEmLAN/embed_node_clf_graph_PositionRank10.txt']\n",
        "\n",
        "with open(f'{parent_path}/{categories_file}', 'r') as f:\n",
        "  tags = f.readlines() # \"tags\" will be a 2D list. Each sublist will have the form: nodeID     label\n",
        "\n",
        "if train_classifier:\n",
        "\n",
        "  clf_test_len = len(nodes) # The number of nodes will be the same in each run since we're using the whole graph and thus, all of its nodes\n",
        "\n",
        "  for ef in embed_files:\n",
        "    X = []\n",
        "    Y = []\n",
        "    new_vector = get_vectors_from_file(ef)\n",
        "\n",
        "    for jk in range(0, clf_test_len):\n",
        "      if str(jk) in nodes: # If the index \"jk\" is a node\n",
        "        tag_list = tags[jk].strip().split() # For node \"jk\", take this info: jk     label\n",
        "        # Y.append([(int)(i) for i in tags])\n",
        "        lli = [str(i) for i in tag_list] # For node \"jk\", lli will contain all of its labels\n",
        "        if len(lli) != 0:\n",
        "          if np.array(new_vector[jk]).any() != np.array(zero_list).any(): # If there is no zero value in the embedding of \"jk\"\n",
        "            X.append(jk)\n",
        "            Y.append(lli[1:][0]) # Take the first label (if there are multiple) of node \"jk\"\n",
        "\n",
        "    # This part of the code uses only the X and Y lists created above\n",
        "    mi = {}\n",
        "    ma = {}\n",
        "    li1 = []\n",
        "    li2 = []\n",
        "    with open(f'{parent_path}/Results/DeepEmLAN/{node_clf_results_file}', 'a') as f:\n",
        "      f.write(f\"{ef.split('/')[-1]} \\n\")\n",
        "      print(ef.split('/')[-1])\n",
        "      for i in range(0, len(clf_ratio)): # Experiment with each ratio\n",
        "        for j in range(0, clf_num): # clf_num = 5\n",
        "\n",
        "          clf = Classifier(vectors=new_vector, # All node embeddings\n",
        "                          clf=LogisticRegression(max_iter=1000))\n",
        "\n",
        "          result = clf.split_train_evaluate(X, Y, clf_ratio[i])\n",
        "\n",
        "          # Results\n",
        "          li1.append(result['micro'])\n",
        "          li2.append(result['macro'])\n",
        "\n",
        "        mi[str(str(clf_ratio[i]) + '-micro')] = sum(li1) / clf_num\n",
        "        ma[str(str(clf_ratio[i]) + '-macro')] = sum(li2) / clf_num\n",
        "\n",
        "        print(mi)\n",
        "        print(ma)\n",
        "        print()\n",
        "\n",
        "        f.writelines(str(str(mi)+str(ma)))\n",
        "        f.write('\\n')\n",
        "\n",
        "        # Reinitialize the dictionaries and lists\n",
        "        mi = {}\n",
        "        ma = {}\n",
        "        li1 = []\n",
        "        li2 = []\n",
        "\n",
        "    gc.collect()"
      ],
      "metadata": {
        "id": "EHXB5pVjGMab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Multiple Executions***"
      ],
      "metadata": {
        "id": "Ys29692wH8_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for gf in ['graph.txt']: #for gf in split_graph_files: # For link prediction. For node classification just use: for gf in ['graph.txt']:\n",
        "\n",
        "  # Create the edge list. Store the unique nodes in the list \"nodes\"\n",
        "  with open(f'{parent_path}/{gf}', 'r') as f:\n",
        "    eedges = f.readlines()\n",
        "\n",
        "  edge_list = []\n",
        "  nodes = []\n",
        "  for ee in eedges:\n",
        "    edge_list.append(list(ee.split()))\n",
        "  for ll in edge_list:\n",
        "    for ed in ll:\n",
        "      if ed not in nodes:\n",
        "        nodes.append(ed)\n",
        "      else:\n",
        "        continue\n",
        "\n",
        "  for t, txtf in enumerate(data_text_files):\n",
        "\n",
        "    MAX_LEN = MAX_LENS[t]\n",
        "    print(f'The maximum length is: {MAX_LEN}')\n",
        "\n",
        "    # Create the dataset. It seems that dataSet() takes the full graph. Perhaps because it is meant for node classification and not for link prediction.\n",
        "    # In link prediction, other NE methods split the graph and the dataSet() takes only a portion of the graph\n",
        "    data = dataSet(f'{parent_path}/{txtf}', f'{parent_path}/{gf}', label_dic)\n",
        "\n",
        "    # Logging the execution details\n",
        "    with open(f'{parent_path}/Results/DeepEmLAN/{log_file}', 'a') as f:\n",
        "        f.write(f'Processing graph: {gf}, text: {txtf}\\n')\n",
        "\n",
        "    print(f'Processing graph: {gf}, text: {txtf}')\n",
        "\n",
        "    # Train the model for \"num_epoch\" epochs\n",
        "    model = Model(data, 1)\n",
        "\n",
        "    start_time = datetime.now()\n",
        "    for i in range(num_epoch):\n",
        "      model.train_one_epoch()\n",
        "\n",
        "    vectors, available_vectors = model.get_embedding() # Get the node embeddings\n",
        "\n",
        "\n",
        "    # Store all the node ids with the same label as node \"ii\".\n",
        "    node_nei_list = {}\n",
        "    one_node_edges = []\n",
        "    for ii in nodes: # For each node \"ii\"\n",
        "      for ed in edge_list: # For each edge \"ed\"\n",
        "        if ii in ed: # If node \"ii\" is in \"ed\". This takes the one-hop neighborhood of \"ii\"\n",
        "          if label_dic[ii] == label_dic[ed[0]] and ii not in one_node_edges: # If the label of \"ii\" is the same as the label of \"ed[0]\" and \"ii\" not in \"one_node_edges\"\n",
        "            one_node_edges.append(ed[0])\n",
        "          if label_dic[ii] == label_dic[ed[1]] and ii not in one_node_edges:\n",
        "            one_node_edges.append(ed[1])\n",
        "        else:\n",
        "          pass\n",
        "      node_nei_list[ii] = one_node_edges # Node \"ii\" has the same label with nodes in \"one_node_edges\"\n",
        "      one_node_edges = []\n",
        "\n",
        "    # Change the vector representation of each node\n",
        "    new_vector = {}\n",
        "    one_node_new_vec = []\n",
        "    for ve in vectors.keys(): # For each node that has an embedding in \"vectors\"\n",
        "      if str(ve) in node_nei_list:\n",
        "        for nnl in node_nei_list[str(ve)]: # Take the nodes with the same label as node \"ve\"\n",
        "          one_node_new_vec.append(vectors[int(nnl)]) # Append the embeddings of all nodes with the same label as \"ve\"\n",
        "        #one_node_new_vec = np.array(one_node_new_vec).sum(axis=0)/len(node_nei_list[str(ve)]) # Take the mean\n",
        "\n",
        "\n",
        "        if one_node_new_vec: # Check if one_node_new_vec has elements before calculating the mean to avoid ZeroDivisionError\n",
        "          one_node_new_vec = np.array(one_node_new_vec).sum(axis=0)/len(node_nei_list[str(ve)]) # Take the mean\n",
        "        else: # Assign zero_list if the node has no neighbors with the same label\n",
        "          one_node_new_vec = zero_list\n",
        "\n",
        "        # The new vector for node \"ve\" will be the mean of all the embeddings of the one-hop neighbors with the same label as \"ve\"\n",
        "        new_vector[ve] = one_node_new_vec\n",
        "        one_node_new_vec = []\n",
        "      else:\n",
        "        new_vector[ve] = zero_list\n",
        "\n",
        "    end_time = datetime.now()\n",
        "    print(f'Time: {((end_time - start_time).total_seconds()) / 60.0} min')\n",
        "    model.close_session()\n",
        "\n",
        "    with open(f'{parent_path}/Results/DeepEmLAN/{log_file}', 'a') as f:\n",
        "      f.write(f'Time: {((end_time - start_time).total_seconds()) / 60.0} min\\n')\n",
        "\n",
        "    # Save embeddings with a unique name\n",
        "    #embed_file = f\"{parent_path}/Results/DeepEmLAN/embed_link_pred_{gf.split('.')[0]}_{tf.split('.')[0]}.txt\"\n",
        "    embed_file = f\"{parent_path}/Results/DeepEmLAN/embed_node_clf_{gf.split('.')[0]}_{txtf.split('.')[0]}.txt\"\n",
        "\n",
        "    ''' with open(embed_file, 'wb') as f:\n",
        "      for node_id, node_vec in new_vector.items():\n",
        "        if available_vectors[node_id] == 1:\n",
        "            f.write((' '.join(map(str, node_vec)) + '\\n').encode())\n",
        "        else:\n",
        "            f.write('\\n'.encode()) '''\n",
        "\n",
        "    with open(embed_file, 'wb') as f:\n",
        "      for node_id, node_vec in new_vector.items():\n",
        "        f.write((' '.join(map(str, node_vec)) + '\\n').encode())\n",
        "\n",
        "    # Log completion\n",
        "    with open(f'{parent_path}/Results/DeepEmLAN/{log_file}', 'a') as f:\n",
        "        f.write(f'Embeddings saved to: {embed_file}\\n')\n",
        "\n",
        "    gc.collect()\n"
      ],
      "metadata": {
        "id": "2G6xeAI_H_3I",
        "outputId": "f2455696-9f55-4200-80e6-0b0ddeae7fe3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The maximum length is: 411\n",
            "Processing graph: graph.txt, text: data.txt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Dimensions must be equal, but are 26304 and 411 for '{{node model/BroadcastTo_4}} = BroadcastTo[T=DT_FLOAT, Tidx=DT_INT32](model/add_1, model/BroadcastTo_4/shape)' with input shapes: [26304,100], [3] and with input tensors computed as partial shapes: input[1] = [64,411,100].",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-5475dd90b1ec>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# Train the model for \"num_epoch\" epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-baae8de29b93>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, order)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0minitializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGlorotNormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_seed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-baae8de29b93>\u001b[0m in \u001b[0;36mbuild_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 tf.nn.embedding_lookup(self.context_embed, tf.cast(self.Node_na, tf.int32)), 1)\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mR_AB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-baae8de29b93>\u001b[0m in \u001b[0;36mconv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;31m# Compute final representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0mR_AB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mu_A\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mu_B\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mR_AB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mR_AB\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_LEN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_size\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mMAX_LEN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_size\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m         \u001b[0;31m#print(\"R_AB shape:\", R_AB.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;31m#print(\"self.R_AB shape:\", self.R_AB.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mbroadcast_to\u001b[0;34m(input, shape, name)\u001b[0m\n\u001b[1;32m    907\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 909\u001b[0;31m     _, _, _op, _outputs = _op_def_library._apply_op_helper(\n\u001b[0m\u001b[1;32m    910\u001b[0m         \"BroadcastTo\", input=input, shape=shape, name=name)\n\u001b[1;32m    911\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    794\u001b[0m       \u001b[0;31m# Add Op to graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m       \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 796\u001b[0;31m       op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n\u001b[0m\u001b[1;32m    797\u001b[0m                                  \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m                                  attrs=attr_protos, op_def=op_def)\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m   2699\u001b[0m     \u001b[0;31m# Session.run call cannot occur between creating and mutating the op.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2700\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mutation_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2701\u001b[0;31m       ret = Operation.from_node_def(\n\u001b[0m\u001b[1;32m   2702\u001b[0m           \u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2703\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mfrom_node_def\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m     \u001b[0;31m# Initialize c_op from node_def and other inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1196\u001b[0;31m     \u001b[0mc_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_c_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol_input_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mop_def\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1197\u001b[0m     \u001b[0mself\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSymbolicTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1198\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs, op_def, extract_traceback)\u001b[0m\n\u001b[1;32m   1054\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m   \u001b[0;31m# Record the current Python stack trace as the creating stacktrace of this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Dimensions must be equal, but are 26304 and 411 for '{{node model/BroadcastTo_4}} = BroadcastTo[T=DT_FLOAT, Tidx=DT_INT32](model/add_1, model/BroadcastTo_4/shape)' with input shapes: [26304,100], [3] and with input tensors computed as partial shapes: input[1] = [64,411,100]."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***IMPORTANT*** ==> Because the maximum length is always the same (100), the computation time, given different data text files, will always be approximately the same.\n",
        "\n",
        "\n",
        "***Hypothesis*** ==> When using the entire abstract, which consists of more that 100(MAX_LEN) words, we see a reduction in performance when compared to the keywords/keyphrases. This happens because the maximum words in all lines of the keyword files are less than 100 so we don't lose any information whereas the maximum words in all lines of the original data file (data.txt) exceed 100. Even if there are abstracts in data.txt with less than 100 words, they must be very few."
      ],
      "metadata": {
        "id": "oCSSGchYI_iP"
      }
    }
  ]
}