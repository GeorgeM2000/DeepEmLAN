{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GeorgeM2000/DeepEmLAN/blob/main/DeepEmLAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64d9j-gmBWw1"
      },
      "source": [
        "# ***Libraries & Tools***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7PFt1_A0Baw_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import gc\n",
        "import sys\n",
        "import re\n",
        "import tensorflow as tf; tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "from math import pow\n",
        "from datetime import datetime\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from time import time\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup for GPU utilization"
      ],
      "metadata": {
        "id": "1-HQhjy-QV3n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rfq65TN5TEmC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b4dda8b-3142-4e78-b71b-98e2275ed665"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n",
            "GPU is ready for use!\n"
          ]
        }
      ],
      "source": [
        "# Check if GPU is available\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "\n",
        "# Optional: Set GPU memory growth to avoid over-allocation\n",
        "\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(\"GPU is ready for use!\")\n",
        "    except RuntimeError as e:\n",
        "        print(e)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Is TensorFlow using GPU?\", tf.test.is_built_with_cuda())  # Should return True\n",
        "print(\"GPU device:\", tf.test.gpu_device_name())  # Should return something like /device:GPU:0"
      ],
      "metadata": {
        "id": "UC_KoTjAVKEm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06db6159-8e37-42a6-8f5b-9166738e9876"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is TensorFlow using GPU? True\n",
            "GPU device: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.config.set_visible_devices(tf.config.list_physical_devices('GPU')[0], 'GPU')"
      ],
      "metadata": {
        "id": "RIyQhZi1VMF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHlh50FCBjuI"
      },
      "source": [
        "# ***Global Variables & General Functionality***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzOOjI5qBCJP"
      },
      "outputs": [],
      "source": [
        "MAX_LEN = 100 # Default value\n",
        "MAX_LENS = [] # For variable MAX_LEN\n",
        "neg_table_size = 1000000\n",
        "NEG_SAMPLE_POWER = 0.75\n",
        "batch_size = 64\n",
        "num_epoch = 50\n",
        "embed_size = 200\n",
        "lr = 1e-3\n",
        "\n",
        "tag_size = 5 # Number of categories. Default: 7\n",
        "Beta = 1.0\n",
        "negative_ratio = 1\n",
        "edge_num = 366792 # Options: 1) 470994 2) 5214 3) 366792 Default: 5429\n",
        "\n",
        "# Importance coefficients\n",
        "rho1 = 0.3\n",
        "rho2 = 0.1\n",
        "rho3 = 0.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f14uDaQK3o5c"
      },
      "outputs": [],
      "source": [
        "clf_ratio = [0.15, 0.45, 0.75]\n",
        "clf_num = 5\n",
        "train_classifier = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2AKcSCx3tyZ"
      },
      "outputs": [],
      "source": [
        "node_clf_results_file = 'DeepEmLAN_Node_Clf_Res.txt'\n",
        "link_pred_results_file = 'DeepEmLAN_Link_Pred_Res.txt'\n",
        "log_file = 'DeepEmLAN_Execution_Logs.txt'\n",
        "\n",
        "split_graph_file = 'sgraph15.txt'\n",
        "test_graph_file = 'tgraph85.txt'\n",
        "split_graph_files = ['sgraph15.txt', 'sgraph45.txt', 'sgraph75.txt']\n",
        "test_graph_files = ['tgraph85.txt', 'tgraph55.txt', 'tgraph25.txt']\n",
        "\n",
        "\n",
        "dataset_name = \"arxiv\"\n",
        "categories_file = 'group-v3.txt'\n",
        "data_text_file = \"YAKE.txt\" # For single execution\n",
        "graph_file = \"graph.txt\" # For single execution\n",
        "data_text_files = [\"data-v3-500.txt\", \"data-v3-500C.txt\", \"YAKE10.txt\", \"YAKE5.txt\", \"RAKE10.txt\", \"RAKE5.txt\", \"RAKE10C.txt\", \"RAKE5C.txt\", \"TFIDF10.txt\", \"TFIDF5.txt\", \"PosR5.txt\",\n",
        "                   \"PosR10.txt\", \"TextR5.txt\", \"TextR10.txt\", \"TopicR5.txt\", \"TopicR10.txt\"]\n",
        "#parent_path = f'Datasets/{dataset_name}/graph-v2'\n",
        "parent_path = '/content'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NX6wAiiWMt7r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5671a77-8c90-4d8b-8dbe-460d0a9d0ac7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== data.txt ===\n",
            "Max word count: 410\n",
            "Min word count: 30\n",
            "\n",
            "=== YAKE.txt ===\n",
            "Max word count: 15\n",
            "Min word count: 14\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Find the maximum number of words from each data text file\n",
        "for txtf in ['data.txt', 'YAKE.txt']: # Options: 1) ['data.txt'] 2) data_text_files:\n",
        "  max_word_count = 0\n",
        "  min_word_count = float('inf')\n",
        "\n",
        "  with open(f'{parent_path}/{txtf}', 'r', encoding='utf-8') as f:\n",
        "      for line in f:\n",
        "          word_count = len(re.findall(r\"\\b\\w+\\b\", line))\n",
        "\n",
        "          if word_count > max_word_count:\n",
        "              max_word_count = word_count\n",
        "\n",
        "          if word_count < min_word_count:\n",
        "              min_word_count = word_count\n",
        "\n",
        "  MAX_LENS.append(max_word_count+1)\n",
        "  print(f'=== {txtf} ===')\n",
        "  print(\"Max word count:\", max_word_count)\n",
        "  print(\"Min word count:\", min_word_count)\n",
        "  print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJh07WWIykfY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9af9af1d-5623-411b-9254-7c307bed7cc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== data.txt ===\n",
            "Mean word count: 90.44180939833113\n",
            "\n",
            "=== YAKE.txt ===\n",
            "Mean word count: 14.99824330259113\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Find the average number of words from each data text file\n",
        "for txtf in ['data.txt', 'YAKE.txt']: # Options: 1) ['data.txt'] 2) data_text_files:\n",
        "    total_word_count = 0\n",
        "    total_lines = 0\n",
        "\n",
        "    with open(f'{parent_path}/{txtf}', 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            total_word_count += len(re.findall(r\"\\b\\w+\\b\", line))\n",
        "            total_lines += 1\n",
        "\n",
        "    mean_word_count = total_word_count / total_lines if total_lines > 0 else 0\n",
        "    MAX_LENS.append(round(mean_word_count))\n",
        "    print(f'=== {txtf} ===')\n",
        "    print(\"Mean word count:\", mean_word_count)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5KRANNNE_o0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "683e4df8-3769-4122-d34e-662a3b1f2e54"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[411, 16]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "MAX_LENS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7M5mje-xMx00"
      },
      "outputs": [],
      "source": [
        "MAX_LEN = MAX_LENS[-1] # For single execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UpBHSimHTyrC"
      },
      "outputs": [],
      "source": [
        "def get_vectors_from_file(file_path):\n",
        "  vectors = {}\n",
        "\n",
        "  with open(f'{file_path}', \"r\", encoding='utf-8') as f:\n",
        "      for idx, line in enumerate(f):\n",
        "          vector = list(map(float, line.strip().split()))  # Convert to list of floats\n",
        "          vectors[idx] = vector  # Assign embedding to node idx\n",
        "\n",
        "  return vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQLMk6xzBRgr"
      },
      "source": [
        "# ***Dataset***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UeQ4pb6qBKDF"
      },
      "outputs": [],
      "source": [
        "class dataSet:\n",
        "\tdef __init__(self, text_path, graph_path, label_dic, text_filename=None):\n",
        "\n",
        "\t\tself.text_filename = text_filename if text_filename else \"\"\n",
        "\t\ttext_file, graph_file = self.load(text_path, graph_path)\n",
        "\t\tself.edges = self.load_edges(graph_file)\n",
        "\t\tself.label_dic = label_dic # This dictionary contains the label of each node\n",
        "\t\tself.text, self.num_vocab, self.num_nodes = self.load_text(text_file)\n",
        "\t\tself.negative_table = InitNegTable(self.edges) # \"negative_table\" uses only the available edges (depends soley on the graph provided as a parameter)\n",
        "\n",
        "\n",
        "\tdef load(self, text_path, graph_path):\n",
        "\t\ttext_file = open(text_path, 'rb').readlines()\n",
        "\t\t''' for a in range(0, len(text_file)):\n",
        "\t\t\ttext_file[a] = str(text_file[a]) '''\n",
        "\t\tgraph_file = open(graph_path, 'rb').readlines()\n",
        "\t\treturn text_file, graph_file\n",
        "\n",
        "\tdef load_edges(self, graph_file):\n",
        "\t\tedges = [] # \"edges\" will be a 2D list. Each sublist will have two values: [node1, node2]\n",
        "\t\tfor i in graph_file:\n",
        "\t\t\tedges.append(list(map(int, i.strip().decode().split()))) # Original: map(int, i.split()) --> This returns an object that is not subscriptable\n",
        "\n",
        "\t\tprint(\"Total load %d edges.\" % len(edges))\n",
        "\t\treturn edges\n",
        "\n",
        "\t# Use this function when the text file has 1s and 0s in each line representing presence or absence of a word (or something else)\n",
        "\t''' def load_text(self, text_file):\n",
        "\t\ttext_one = [0] * MAX_LEN # Create a zeros vector with \"MAX_LEN\" length\n",
        "\t\ttext = []\n",
        "\t\tfor te in text_file:\n",
        "\t\t\ti = 0\n",
        "\t\t\tj = 0\n",
        "\t\t\ttte = te.split()[1:]\n",
        "\t\t\tfor t in tte:\n",
        "\t\t\t\tif t == '1.0' or t == '1' or t == '1.':\n",
        "\t\t\t\t\ttext_one[j] = i+1\n",
        "\t\t\t\t\tj = j+1\n",
        "\t\t\t\ti = i+1\n",
        "\t\t\ttext.append(text_one)\n",
        "\t\t\ttext_one = [0] * MAX_LEN\n",
        "\n",
        "\t\ttext = np.array(text)\n",
        "\t\tnum_vocab = len(text_file[0].split()) + 1\n",
        "\t\tself.num_nodes = len(text)\n",
        "\t\treturn text, num_vocab, self.num_nodes '''\n",
        "\n",
        "\t# load_text() method using TextVectorization() from Keras\n",
        "\t''' def load_text(self, text_file):\n",
        "\t\tvectorize_layer = tf.keras.layers.TextVectorization(\n",
        "\t\t\t\tmax_tokens=None,  # Set a limit if needed\n",
        "\t\t\t\toutput_mode='int',\n",
        "\t\t\t\toutput_sequence_length=MAX_LEN\n",
        "\t\t)\n",
        "\n",
        "\t\ttext_data = [line.strip() for line in text_file]\n",
        "\t\ttext_data_size = len(text_data)\n",
        "\n",
        "\t\tif \"data-v3-500\" in self.text_filename:\n",
        "\t\t\tbatch_size = int(text_data_size / 10)\n",
        "\t\t\tranges = []\n",
        "\t\t\tstart = 0\n",
        "\t\t\twhile start < text_data_size:\n",
        "\t\t\t\t\tend = min(start + batch_size, text_data_size)  # Ensure the last range includes all remaining abstracts\n",
        "\t\t\t\t\tranges.append([start, end])\n",
        "\t\t\t\t\tstart = end\n",
        "\n",
        "\t\t\tranges[-2][1] = ranges[-1][1]\n",
        "\t\t\tdel ranges[-1]\n",
        "\n",
        "\t\t\tfor range in ranges:\n",
        "\t\t\t\tvectorize_layer.adapt(text_data[range[0]:range[1]])\n",
        "\t\telse:\n",
        "\t\t\tvectorize_layer.adapt(text_data)\n",
        "\n",
        "\t\ttext = vectorize_layer(text_data).numpy()\n",
        "\n",
        "\t\tnum_vocab = len(vectorize_layer.get_vocabulary())\n",
        "\t\tnum_nodes = len(text)\n",
        "\n",
        "\t\treturn text, num_vocab, num_nodes '''\n",
        "\n",
        "\t# load_text() method using Tokenizer\n",
        "\tdef load_text(self, text_file):\n",
        "\t\t#text_data = [line.strip() for line in text_file]\n",
        "\t\ttext_data = [line.decode('utf-8').strip() for line in text_file]\n",
        "\n",
        "\t\ttokenizer = Tokenizer(oov_token=None) # Default: Tokenizer()\n",
        "\t\ttokenizer.fit_on_texts(text_data)\n",
        "\n",
        "\t\ttext = tokenizer.texts_to_sequences(text_data)\n",
        "\t\ttext = pad_sequences(text, maxlen=MAX_LEN, padding=\"post\", truncating='post') # Default: pad_sequences(text, maxlen=MAX_LEN, padding=\"post\")\n",
        "\n",
        "\t\tnum_vocab = len(tokenizer.word_index) + 1  # +1 for padding token\n",
        "\t\tnum_nodes = len(text)\n",
        "\t\treturn text, num_vocab, num_nodes\n",
        "\n",
        "\n",
        "\tdef negative_sample(self, edges):\n",
        "\t\tnode1, node2 = list(zip(*edges))[0:2] # Original: zip(*edges)[0:2] --> Leads to error\n",
        "\t\tneg_sample_edges = []\n",
        "\t\tfunc = lambda: self.negative_table[random.randint(0, neg_table_size-1)]\n",
        "\t\tfor i in range(len(edges)):\n",
        "\t\t\tneg_node = func()\n",
        "\t\t\twhile self.label_dic[str(node1[i])] == self.label_dic[str(neg_node)] or self.label_dic[str(node2[i])] == \\\n",
        "\t\t\t\t\tself.label_dic[str(neg_node)]:\n",
        "\t\t\t\tneg_node = func()\n",
        "\t\t\tneg_sample_edges.append(neg_node)\n",
        "\n",
        "\t\treturn neg_sample_edges\n",
        "\n",
        "\tdef generate_batches(self, mode=None):\n",
        "\t\tr_one_hot = np.zeros(tag_size, dtype=float)\n",
        "\t\tbr_one_hot = np.ones(tag_size, dtype=float)\n",
        "\t\tnum_batch = len(self.edges) // batch_size # Original: len(self.edges) / batch_size --> The result is a float which cannot be used for list slicing\n",
        "\t\tedge_l = self.edges\n",
        "\t\tfor i in range(0, edge_num):\n",
        "\t\t\tr_one_hot[int(self.label_dic[str(edge_l[i][0])])] = 1.0\n",
        "\t\t\tr_one_hot[int(self.label_dic[str(edge_l[i][1])])] = 1.0\n",
        "\t\t\tbr_one_hot[int(self.label_dic[str(edge_l[i][0])])] = Beta\n",
        "\t\t\tbr_one_hot[int(self.label_dic[str(edge_l[i][1])])] = Beta\n",
        "\t\t\tedge_l[i].append(r_one_hot)\n",
        "\t\t\tedge_l[i].append(br_one_hot)\n",
        "\t\t\tr_one_hot = np.zeros(tag_size, dtype=float)\n",
        "\t\t\tbr_one_hot = np.ones(tag_size, dtype=float)\n",
        "\t\tif mode == 'add':\n",
        "\t\t\tnum_batch += 1\n",
        "\t\t\tedge_l.extend(edge_l[:(batch_size - len(self.edges) // batch_size)])\n",
        "\t\tif mode != 'add':\n",
        "\t\t\trandom.shuffle(edge_l)\n",
        "\n",
        "\t\tsample_edges = edge_l[:num_batch * batch_size]\n",
        "\t\tsample_neg = self.negative_sample(sample_edges)\n",
        "\n",
        "\t\tshuffle_edges = []\n",
        "\t\tshuffle_relation = []\n",
        "\t\tshuffle_brelation = []\n",
        "\t\t# shuffle_edgetags = []\n",
        "\t\tfor edge in sample_edges:\n",
        "\t\t\tshuffle_edges.append([edge[0], edge[1]])\n",
        "\t\t\tshuffle_relation.append(edge[2])\n",
        "\t\t\tshuffle_brelation.append(edge[3])\n",
        "\t\tbatches_edges = []\n",
        "\t\tbatches_re = []\n",
        "\t\tbatches_neg = []\n",
        "\t\tbatches_br = []\n",
        "\t\tfor j in range(num_batch):\n",
        "\t\t\tbatches_edges.append(shuffle_edges[j * batch_size:(j + 1) * batch_size])\n",
        "\t\t\tbatches_re.append(np.array(shuffle_relation[j * batch_size:(j + 1) * batch_size]))\n",
        "\t\t\tbatches_neg.append(sample_neg[j * batch_size:(j + 1) * batch_size])\n",
        "\t\t\tbatches_br.append(np.array(shuffle_brelation[j * batch_size:(j + 1) * batch_size]))\n",
        "\n",
        "\t\treturn batches_edges, batches_re, batches_neg, batches_br"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mt0xoLN7BuZO"
      },
      "source": [
        "# ***DeepEmLAN***"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Version-1 (Original)"
      ],
      "metadata": {
        "id": "zVJroyDvzCok"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gZqZFbABvuX"
      },
      "outputs": [],
      "source": [
        "class Model:\n",
        "    def __init__(self, data, order):\n",
        "        tf.compat.v1.reset_default_graph() # With every new model all the variables used in a previous model will be reset\n",
        "        self.cur_epoch = 0\n",
        "        self.vocab_size = data.num_vocab\n",
        "        self.num_nodes = data.num_nodes\n",
        "        self.order = order\n",
        "        self.data = data\n",
        "        self.sess = tf.compat.v1.Session()\n",
        "        self.n_hidden_1 = 64\n",
        "        self.n_hidden_2 = 32\n",
        "        self.output_size = tag_size\n",
        "\n",
        "        cur_seed = random.getrandbits(32)\n",
        "        initializer = tf.keras.initializers.GlorotNormal(seed=cur_seed)\n",
        "        with tf.compat.v1.variable_scope(\"model\", reuse=None, initializer=initializer):\n",
        "            self.build_graph()\n",
        "        self.sess.run(tf.compat.v1.global_variables_initializer())\n",
        "\n",
        "    def close_session(self):\n",
        "        self.sess.close()\n",
        "\n",
        "    def build_graph(self):\n",
        "            # '''hyperparameter'''\n",
        "        with tf.name_scope('read_inputs') as scope:\n",
        "            self.Text_a = tf.compat.v1.placeholder(tf.int32, [batch_size, MAX_LEN], name='Ta' + str(self.order))\n",
        "            self.Text_b = tf.compat.v1.placeholder(tf.int32, [batch_size, MAX_LEN], name='Tb' + str(self.order))\n",
        "            self.Text_pos = tf.compat.v1.placeholder(tf.int32, [batch_size, MAX_LEN], name='Tpos'+ str(self.order))\n",
        "            self.Text_neg = tf.compat.v1.placeholder(tf.int32, [batch_size, MAX_LEN], name='Tneg'+ str(self.order))\n",
        "            self.Node_a = tf.compat.v1.placeholder(tf.int32, [batch_size], name='n1' + str(self.order))\n",
        "            self.Node_b = tf.compat.v1.placeholder(tf.int32, [batch_size], name='n2' + str(self.order))\n",
        "            self.Node_pa = tf.compat.v1.placeholder(tf.int32, [batch_size], name='n4' + str(self.order))\n",
        "            self.Node_na = tf.compat.v1.placeholder(tf.int32, [batch_size], name='n5' + str(self.order))\n",
        "            self.relation = tf.compat.v1.placeholder(tf.float32, [batch_size, tag_size], name='r' + str(self.order))\n",
        "            self.brelation = tf.compat.v1.placeholder(tf.float32, [batch_size, tag_size], name='br' + str(self.order))\n",
        "\n",
        "        with tf.name_scope('initialize_para') as scope:\n",
        "            self.weights = {\n",
        "                'encoder_h1'+ str(self.order): tf.Variable(tf.compat.v1.random_normal([embed_size // 2, self.n_hidden_1])),\n",
        "                'encoder_h2'+ str(self.order): tf.Variable(tf.compat.v1.random_normal([self.n_hidden_1, self.n_hidden_2])),\n",
        "                'encoder_h3'+ str(self.order): tf.Variable(tf.compat.v1.random_normal([self.n_hidden_2, self.output_size]))\n",
        "            }\n",
        "            cur_seed = random.getrandbits(32)\n",
        "            self.biases = {\n",
        "                'encoder_b1'+ str(self.order): tf.Variable(tf.compat.v1.random_normal([self.n_hidden_1])),\n",
        "                'encoder_b2'+ str(self.order): tf.Variable(tf.compat.v1.random_normal([self.n_hidden_2])),\n",
        "                'encoder_b3'+ str(self.order): tf.Variable(tf.compat.v1.random_normal([self.output_size]))\n",
        "            }\n",
        "\n",
        "        with tf.name_scope('initialize_embedding') as scope:\n",
        "            cur_seed = random.getrandbits(32)\n",
        "            self.context_embed = tf.compat.v1.get_variable(name=\"context_embeddings\"+ str(self.order),\n",
        "                                                 shape=[self.num_nodes, embed_size // 2],\n",
        "                                                 initializer=tf.keras.initializers.GlorotNormal(seed=cur_seed))\n",
        "\n",
        "            self.text_embed = tf.compat.v1.get_variable(name=\"text_embeddings\"+ str(self.order),\n",
        "                                              shape=[self.vocab_size, embed_size // 2],\n",
        "                                              initializer=tf.keras.initializers.GlorotNormal(seed=cur_seed))\n",
        "\n",
        "            self.node_embed = tf.compat.v1.get_variable(name=\"embeddings\" + str(self.order),\n",
        "                                              shape=[self.num_nodes, embed_size // 2],\n",
        "                                              initializer=tf.keras.initializers.GlorotNormal(seed=cur_seed))\n",
        "\n",
        "        with tf.name_scope('lookup_embeddings') as scope:\n",
        "            self.TA = tf.nn.embedding_lookup(self.text_embed, self.Text_a)\n",
        "            self.T_A = tf.expand_dims(self.TA, -1)\n",
        "\n",
        "            self.TB = tf.nn.embedding_lookup(self.text_embed, self.Text_b)\n",
        "            self.T_B = tf.expand_dims(self.TB, -1)\n",
        "\n",
        "            self.Tpos = tf.nn.embedding_lookup(self.text_embed, self.Text_pos)\n",
        "            self.T_POS = tf.expand_dims(self.Tpos, -1)\n",
        "\n",
        "            self.TNEG = tf.nn.embedding_lookup(self.text_embed, self.Text_neg)\n",
        "            self.T_NEG = tf.expand_dims(self.TNEG, -1)\n",
        "\n",
        "            self.N_A = tf.nn.l2_normalize(tf.nn.embedding_lookup(self.node_embed, self.Node_a), 1)\n",
        "            self.N_B = tf.nn.l2_normalize(tf.nn.embedding_lookup(self.node_embed, self.Node_b), 1)\n",
        "            self.N_POS = tf.nn.embedding_lookup(self.node_embed, self.Node_pa)\n",
        "            self.N_NEG = tf.nn.embedding_lookup(self.node_embed, self.Node_na)\n",
        "            self.N_NEG_list = tf.split(self.N_NEG, negative_ratio, 1)\n",
        "\n",
        "            self.pos_nb_context = tf.nn.l2_normalize(\n",
        "                tf.nn.embedding_lookup(self.context_embed, tf.cast(self.Node_b, tf.int32)), 1)\n",
        "            self.pos_ab = tf.nn.l2_normalize(\n",
        "                tf.nn.embedding_lookup(self.node_embed, tf.cast(self.Node_pa, tf.int32)), 1)\n",
        "            self.neg_ab = tf.nn.l2_normalize(\n",
        "                tf.nn.embedding_lookup(self.node_embed, tf.cast(self.Node_na, tf.int32)), 1)\n",
        "            self.neg_ab_context = tf.nn.l2_normalize(\n",
        "                tf.nn.embedding_lookup(self.context_embed, tf.cast(self.Node_na, tf.int32)), 1)\n",
        "\n",
        "        self.R_AB, self.l_predict = self.conv()\n",
        "        self.loss = self.compute_loss(self.order)\n",
        "        optimizer = tf.compat.v1.train.AdamOptimizer(lr)\n",
        "        self.train_op = optimizer.minimize(self.loss)\n",
        "\n",
        "    def train_one_epoch(self):\n",
        "        loss_epoch = 0\n",
        "        batches_edges, batches_re, batches_neg, batches_br = self.data.generate_batches()\n",
        "        num_batch = len(batches_edges)\n",
        "        for i in range(num_batch):\n",
        "            node1, node2 = zip(*batches_edges[i])\n",
        "            node_list4 = batches_neg[i]\n",
        "            batch_r = batches_re[i]\n",
        "            batch_br = batches_br[i]\n",
        "            node_list3 = node2\n",
        "            node1, node2, node_list3, node_list4 = np.array(node1), np.array(node2), \\\n",
        "                                                          np.array(node_list3), np.array(node_list4)\n",
        "            node_list3 = np.transpose(node_list3)\n",
        "            text1, text2 = self.data.text[node1], self.data.text[node2]\n",
        "            text_pos = []\n",
        "            text_neg = []\n",
        "            for npp in node_list3:\n",
        "                text_pos.append(self.data.text[npp])\n",
        "            for nn in node_list4:\n",
        "                text_neg.append(self.data.text[nn])\n",
        "\n",
        "            feed_dict = {\n",
        "                self.Text_a: text1,\n",
        "                self.Text_b: text2,\n",
        "                self.Text_pos: text_pos,\n",
        "                self.Text_neg: text_neg,\n",
        "                self.Node_a: node1,\n",
        "                self.Node_b: node2,\n",
        "                self.Node_pa: node_list3,\n",
        "                self.Node_na: node_list4,\n",
        "                self.relation: batch_r,\n",
        "                self.brelation: batch_br\n",
        "\n",
        "            }\n",
        "\n",
        "            # run the graph\n",
        "            _, loss_batch = self.sess.run([self.train_op, self.loss], feed_dict=feed_dict)\n",
        "            # print loss_batch\n",
        "            loss_epoch += loss_batch\n",
        "        #print str('order' + str(self.order) + ':'), self.cur_epoch, ' loss: ', loss_epoch\n",
        "        self.cur_epoch += 1\n",
        "\n",
        "    def encoder(self, x):\n",
        "        layer_1 = tf.nn.tanh(tf.add(tf.matmul(tf.reshape(x,[batch_size, embed_size // 2]), self.weights['encoder_h1'+ str(self.order)]),\n",
        "                                       self.biases['encoder_b1'+ str(self.order)]))\n",
        "        layer_2 = tf.nn.tanh(tf.add(tf.matmul(layer_1, self.weights['encoder_h2'+ str(self.order)]),\n",
        "                                       self.biases['encoder_b2'+ str(self.order)]))\n",
        "        layer_3 = tf.reshape(tf.nn.tanh(tf.add(tf.matmul(layer_2, self.weights['encoder_h3'+ str(self.order)]),\n",
        "                                       self.biases['encoder_b3'+ str(self.order)])),[batch_size,1,self.output_size])\n",
        "        return layer_3\n",
        "\n",
        "\n",
        "    # CANE's version of conv()\n",
        "    def conv(self):\n",
        "        W2 = tf.Variable(tf.random.truncated_normal([2, embed_size // 2, 1, 100], stddev=0.3))\n",
        "        rand_matrix = tf.Variable(tf.random.truncated_normal([100, 100], stddev=0.3))\n",
        "\n",
        "        convA = tf.nn.conv2d(self.T_A, W2, strides=[1, 1, 1, 1], padding='VALID')\n",
        "        convB = tf.nn.conv2d(self.T_B, W2, strides=[1, 1, 1, 1], padding='VALID')\n",
        "        convPOS = tf.nn.conv2d(self.T_POS, W2, strides=[1, 1, 1, 1], padding='VALID')\n",
        "        convNEG = tf.nn.conv2d(self.T_NEG, W2, strides=[1, 1, 1, 1], padding='VALID')\n",
        "\n",
        "        hA = tf.tanh(tf.squeeze(convA))\n",
        "        hB = tf.tanh(tf.squeeze(convB))\n",
        "        hPOS = tf.tanh(tf.squeeze(convPOS))\n",
        "        hNEG = tf.tanh(tf.squeeze(convNEG))\n",
        "\n",
        "        tmphA = tf.reshape(hA, [batch_size * (MAX_LEN - 1), embed_size // 2])\n",
        "        ha_mul_rand = tf.reshape(tf.matmul(tmphA, rand_matrix), [batch_size, MAX_LEN - 1, embed_size // 2])\n",
        "\n",
        "        r1 = tf.matmul(ha_mul_rand, hB, adjoint_b=True)\n",
        "        r3 = tf.matmul(ha_mul_rand, hNEG, adjoint_b=True)\n",
        "        r4 = tf.matmul(ha_mul_rand, hPOS, adjoint_b=True)\n",
        "        att1 = tf.expand_dims(tf.stack(r1), -1)\n",
        "        att3 = tf.expand_dims(tf.stack(r3), -1)\n",
        "        att4 = tf.expand_dims(tf.stack(r4), -1)\n",
        "\n",
        "        att1 = tf.tanh(att1)\n",
        "        att3 = tf.tanh(att3)\n",
        "        att4 = tf.tanh(att4)\n",
        "\n",
        "\n",
        "        pooled_A = tf.reduce_mean(att1, 2)\n",
        "        pooled_B = tf.reduce_mean(att1, 1)\n",
        "        pooled_POS = tf.reduce_mean(att4, 1)\n",
        "        pooled_NEG = tf.reduce_mean(att3, 1)\n",
        "\n",
        "        a_flat = tf.squeeze(pooled_A)\n",
        "        b_flat = tf.squeeze(pooled_B)\n",
        "        pos_flat = tf.squeeze(pooled_POS)\n",
        "        neg_flat = tf.squeeze(pooled_NEG)\n",
        "\n",
        "        w_A = tf.nn.softmax(a_flat)\n",
        "        w_B = tf.nn.softmax(b_flat)\n",
        "        w_POS = tf.nn.softmax(pos_flat)\n",
        "        w_NEG = tf.nn.softmax(neg_flat)\n",
        "\n",
        "        rep_A = tf.expand_dims(w_A, -1)\n",
        "        rep_B = tf.expand_dims(w_B, -1)\n",
        "        rep_POS = tf.expand_dims(w_POS, -1)\n",
        "        rep_NEG = tf.expand_dims(w_NEG, -1)\n",
        "\n",
        "        hA = tf.transpose(hA, perm=[0, 2, 1])\n",
        "        hB = tf.transpose(hB, perm=[0, 2, 1])\n",
        "        hPOS = tf.transpose(hPOS, perm=[0, 2, 1])\n",
        "        hNEG = tf.transpose(hNEG, perm=[0, 2, 1])\n",
        "\n",
        "        rep1 = tf.matmul(hA, rep_A)\n",
        "        rep2 = tf.matmul(hB, rep_B)\n",
        "        rep3 = tf.matmul(hNEG, rep_NEG)\n",
        "        rep4 = tf.matmul(hPOS, rep_POS)\n",
        "\n",
        "        self.u_A = tf.squeeze(rep1)\n",
        "        self.u_B = tf.squeeze(rep2)\n",
        "        self.u_P = tf.squeeze(rep4)\n",
        "        self.u_N = tf.squeeze(rep3)\n",
        "\n",
        "        R_AB = self.u_A + self.u_B\n",
        "        self.R_AB = tf.reshape(R_AB, [batch_size, embed_size // 2])\n",
        "\n",
        "        l_predict = self.encoder(tf.reshape(self.R_AB, [batch_size, 1, embed_size // 2]))\n",
        "        return R_AB, l_predict\n",
        "\n",
        "    def compute_loss(self,order):\n",
        "\n",
        "        rho1 = 0.3\n",
        "        rho2 = 0.1\n",
        "        rho3 = 0.3\n",
        "        p1 = tf.reduce_sum(tf.multiply(self.u_A, self.u_B), 1)\n",
        "        p1 = tf.math.log(tf.nn.sigmoid(p1) + 0.001)\n",
        "        p2 = tf.reduce_sum(tf.multiply(self.N_A, self.N_B), 1)\n",
        "        p2 = tf.math.log(tf.nn.sigmoid(p2) + 0.001)\n",
        "\n",
        "        p3 = tf.reduce_sum(tf.multiply(self.N_A, self.u_A), 1)\n",
        "        p3 = tf.math.log(tf.nn.sigmoid(p3) + 0.001)\n",
        "        p4 = tf.math.reduce_sum(tf.multiply(self.N_B, self.u_B), 1)\n",
        "        p4 = tf.math.log(tf.nn.sigmoid(p4) + 0.001)\n",
        "\n",
        "\n",
        "        for i in range(0, 1):\n",
        "        # for i in range(0, config.negative_ratio):\n",
        "            u_P1 = tf.reshape(tf.split(self.u_P, negative_ratio, 1)[i], [batch_size, embed_size // 2])\n",
        "            u_N1 = tf.reshape(tf.split(self.u_N, negative_ratio, 1)[i],\n",
        "                              [batch_size, embed_size // 2])\n",
        "            p5 = tf.reduce_sum(tf.multiply(self.u_A, u_P1), 1)\n",
        "            p5 = tf.math.log(tf.nn.sigmoid(p5) + 0.001)\n",
        "            p6 = tf.reduce_sum(tf.multiply(self.u_B, u_P1), 1)\n",
        "            p6 = tf.math.log(tf.nn.sigmoid(p6) + 0.001)\n",
        "            p7 = tf.reduce_sum(tf.multiply(self.u_A, u_N1), 1)\n",
        "            p7 = tf.math.log(tf.nn.sigmoid(-p7) + 0.001)\n",
        "            p8 = tf.reduce_sum(tf.multiply(self.u_B, u_N1), 1)\n",
        "            p8 = tf.math.log(tf.nn.sigmoid(-p8) + 0.001)\n",
        "            p9 = tf.reduce_sum(tf.multiply(self.N_A, self.N_NEG), 1)\n",
        "            p9 = tf.math.log(tf.nn.sigmoid(-p9) + 0.001)\n",
        "            p10 = tf.reduce_sum(tf.multiply(self.N_B, self.N_NEG), 1)\n",
        "            p10 = tf.math.log(tf.nn.sigmoid(-p10) + 0.001)\n",
        "\n",
        "        p11 = tf.reduce_sum(tf.multiply(self.l_predict, self.relation))\n",
        "        p11 = tf.math.log(tf.nn.sigmoid(p11) + 0.001)\n",
        "\n",
        "        p_all = rho1*(p1 + p2 + p5 + p6 + p7 + p8 + p9 + p10) + rho2*(p3 + p4) + rho3 * p11\n",
        "\n",
        "        temp_loss = -tf.reduce_sum(p_all+p11)\n",
        "        self.sample_sum1 = tf.reduce_sum(tf.exp(tf.multiply(self.pos_ab, self.neg_ab)),\n",
        "                                         axis=1)\n",
        "        self.first_loss = tf.reduce_mean(-tf.reduce_sum(tf.multiply(self.N_A, self.N_B), axis=1) +\n",
        "                                         tf.math.log(self.sample_sum1))\n",
        "        self.sample_sum2 = tf.reduce_sum(\n",
        "            tf.exp(tf.multiply(self.pos_ab, self.neg_ab_context)), axis=1)\n",
        "        self.second_loss = tf.reduce_mean(-tf.reduce_sum(tf.multiply(self.N_A, self.pos_nb_context), axis=1) +\n",
        "                                          tf.math.log(self.sample_sum2))\n",
        "        loss = temp_loss + self.first_loss + self.second_loss\n",
        "\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def get_embedding(self):\n",
        "        vectors = {}\n",
        "        available_vectors = {} # Nodes that have an embedding: 1 --- Nodes that don't: 0\n",
        "\n",
        "        zero_list = []\n",
        "        for i in range(0, embed_size):\n",
        "            zero_list.append(0)\n",
        "        zero_list = np.array(zero_list)\n",
        "\n",
        "\n",
        "        embed = [[] for _ in range(self.data.num_nodes)]\n",
        "\n",
        "        batches_edges, batches_re, batches_neg, batches_br = self.data.generate_batches(mode='add')\n",
        "        num_batch = len(batches_edges)\n",
        "        for i in range(num_batch):\n",
        "            node1, node2 = zip(*batches_edges[i])\n",
        "            node_list4 = batches_neg[i]\n",
        "            batch_r = batches_re[i]\n",
        "            batch_br = batches_br[i]\n",
        "\n",
        "            node_list3 = node2\n",
        "            node1, node2, node_list3, node_list4 = np.array(node1), np.array(node2), \\\n",
        "                                                                 np.array(node_list3), np.array(\n",
        "                node_list4)\n",
        "            node_list3 = np.transpose(node_list3)\n",
        "            text1, text2 = self.data.text[node1], self.data.text[node2]\n",
        "            text_pos = []\n",
        "            text_neg = []\n",
        "            for npp in node_list3:\n",
        "                text_pos.append(self.data.text[npp])\n",
        "            for nn in node_list4:\n",
        "                text_neg.append(self.data.text[nn])\n",
        "\n",
        "\n",
        "            feed_dict = {\n",
        "                self.Text_a: text1,\n",
        "                self.Text_b: text2,\n",
        "                self.Text_pos: text_pos,\n",
        "                self.Text_neg: text_neg,\n",
        "                self.Node_a: node1,\n",
        "                self.Node_b: node2,\n",
        "                self.Node_pa: node_list3,\n",
        "                self.Node_na: node_list4,\n",
        "                self.relation: batch_r,\n",
        "                self.brelation: batch_br\n",
        "\n",
        "            }\n",
        "            uA, uB, rAB, NA, NB = self.sess.run([self.u_A, self.u_B, self.R_AB, self.N_A, self.N_B], feed_dict=feed_dict)\n",
        "\n",
        "            for i in range(batch_size):\n",
        "                embed[node1[i]].append(list(NA[i])+list(rAB[i]))  #\n",
        "                embed[node2[i]].append(list(NB[i])+list(rAB[i]))  #\n",
        "\n",
        "        for i in range(self.data.num_nodes):\n",
        "            if embed[i]: # If an embedding exists for node i\n",
        "                tmp=np.sum(embed[i],axis=0)/len(embed[i])\n",
        "                vectors[i]=tmp\n",
        "                available_vectors[i] = 1\n",
        "                #file.write(' '.join(map(str,tmp))+'\\n')\n",
        "            else:\n",
        "                vectors[i]=zero_list\n",
        "                available_vectors[i] = 0\n",
        "\n",
        "        return vectors, available_vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Version-2"
      ],
      "metadata": {
        "id": "Qq7kxUf4zGpz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model:\n",
        "    def __init__(self, data, order):\n",
        "        self.vocab_size = data.num_vocab\n",
        "        self.num_nodes = data.num_nodes\n",
        "        self.order = order\n",
        "        self.n_hidden_1 = 64\n",
        "        self.n_hidden_2 = 32\n",
        "        self.output_size = tag_size\n",
        "        cur_seed = random.getrandbits(32)\n",
        "\n",
        "        with tf.name_scope('read_inputs') as scope:\n",
        "            self.Text_a = tf.compat.v1.placeholder(tf.int32, [batch_size, MAX_LEN], name='Ta' + str(self.order))\n",
        "            self.Text_b = tf.compat.v1.placeholder(tf.int32, [batch_size, MAX_LEN], name='Tb' + str(self.order))\n",
        "            self.Text_pos = tf.compat.v1.placeholder(tf.int32, [batch_size, MAX_LEN], name='Tpos'+ str(self.order))\n",
        "            self.Text_neg = tf.compat.v1.placeholder(tf.int32, [batch_size, MAX_LEN], name='Tneg'+ str(self.order))\n",
        "            self.Node_a = tf.compat.v1.placeholder(tf.int32, [batch_size], name='n1' + str(self.order))\n",
        "            self.Node_b = tf.compat.v1.placeholder(tf.int32, [batch_size], name='n2' + str(self.order))\n",
        "            self.Node_pa = tf.compat.v1.placeholder(tf.int32, [batch_size], name='n4' + str(self.order))\n",
        "            self.Node_na = tf.compat.v1.placeholder(tf.int32, [batch_size], name='n5' + str(self.order))\n",
        "            self.relation = tf.compat.v1.placeholder(tf.float32, [batch_size, tag_size], name='r' + str(self.order))\n",
        "            self.brelation = tf.compat.v1.placeholder(tf.float32, [batch_size, tag_size], name='br' + str(self.order))\n",
        "\n",
        "        with tf.name_scope('initialize_para') as scope:\n",
        "            self.weights = {\n",
        "                'encoder_h1'+ str(self.order): tf.Variable(tf.compat.v1.random_normal([embed_size // 2, self.n_hidden_1])),\n",
        "                'encoder_h2'+ str(self.order): tf.Variable(tf.compat.v1.random_normal([self.n_hidden_1, self.n_hidden_2])),\n",
        "                'encoder_h3'+ str(self.order): tf.Variable(tf.compat.v1.random_normal([self.n_hidden_2, self.output_size]))\n",
        "            }\n",
        "            cur_seed = random.getrandbits(32)\n",
        "            self.biases = {\n",
        "                'encoder_b1'+ str(self.order): tf.Variable(tf.compat.v1.random_normal([self.n_hidden_1])),\n",
        "                'encoder_b2'+ str(self.order): tf.Variable(tf.compat.v1.random_normal([self.n_hidden_2])),\n",
        "                'encoder_b3'+ str(self.order): tf.Variable(tf.compat.v1.random_normal([self.output_size]))\n",
        "            }\n",
        "\n",
        "        with tf.name_scope('initialize_embedding') as scope:\n",
        "            cur_seed = random.getrandbits(32)\n",
        "            self.context_embed = tf.compat.v1.get_variable(name=\"context_embeddings\"+ str(self.order),\n",
        "                                                 shape=[self.num_nodes, embed_size // 2],\n",
        "                                                 initializer=tf.keras.initializers.GlorotNormal(seed=cur_seed))\n",
        "\n",
        "            self.text_embed = tf.compat.v1.get_variable(name=\"text_embeddings\"+ str(self.order),\n",
        "                                              shape=[self.vocab_size, embed_size // 2],\n",
        "                                              initializer=tf.keras.initializers.GlorotNormal(seed=cur_seed))\n",
        "\n",
        "            self.node_embed = tf.compat.v1.get_variable(name=\"embeddings\" + str(self.order),\n",
        "                                              shape=[self.num_nodes, embed_size // 2],\n",
        "                                              initializer=tf.keras.initializers.GlorotNormal(seed=cur_seed))\n",
        "\n",
        "        with tf.name_scope('lookup_embeddings') as scope:\n",
        "            self.TA = tf.nn.embedding_lookup(self.text_embed, self.Text_a)\n",
        "            self.T_A = tf.expand_dims(self.TA, -1)\n",
        "\n",
        "            self.TB = tf.nn.embedding_lookup(self.text_embed, self.Text_b)\n",
        "            self.T_B = tf.expand_dims(self.TB, -1)\n",
        "\n",
        "            self.Tpos = tf.nn.embedding_lookup(self.text_embed, self.Text_pos)\n",
        "            self.T_POS = tf.expand_dims(self.Tpos, -1)\n",
        "\n",
        "            self.TNEG = tf.nn.embedding_lookup(self.text_embed, self.Text_neg)\n",
        "            self.T_NEG = tf.expand_dims(self.TNEG, -1)\n",
        "\n",
        "            self.N_A = tf.nn.l2_normalize(tf.nn.embedding_lookup(self.node_embed, self.Node_a), 1)\n",
        "            self.N_B = tf.nn.l2_normalize(tf.nn.embedding_lookup(self.node_embed, self.Node_b), 1)\n",
        "            self.N_POS = tf.nn.embedding_lookup(self.node_embed, self.Node_pa)\n",
        "            self.N_NEG = tf.nn.embedding_lookup(self.node_embed, self.Node_na)\n",
        "            self.N_NEG_list = tf.split(self.N_NEG, negative_ratio, 1)\n",
        "\n",
        "            self.pos_nb_context = tf.nn.l2_normalize(\n",
        "                tf.nn.embedding_lookup(self.context_embed, tf.cast(self.Node_b, tf.int32)), 1)\n",
        "            self.pos_ab = tf.nn.l2_normalize(\n",
        "                tf.nn.embedding_lookup(self.node_embed, tf.cast(self.Node_pa, tf.int32)), 1)\n",
        "            self.neg_ab = tf.nn.l2_normalize(\n",
        "                tf.nn.embedding_lookup(self.node_embed, tf.cast(self.Node_na, tf.int32)), 1)\n",
        "            self.neg_ab_context = tf.nn.l2_normalize(\n",
        "                tf.nn.embedding_lookup(self.context_embed, tf.cast(self.Node_na, tf.int32)), 1)\n",
        "\n",
        "        self.R_AB, self.l_predict = self.conv()\n",
        "        self.loss = self.compute_loss(self.order)\n",
        "\n",
        "    def encoder(self, x):\n",
        "        layer_1 = tf.nn.tanh(tf.add(tf.matmul(tf.reshape(x,[batch_size, embed_size // 2]), self.weights['encoder_h1'+ str(self.order)]),\n",
        "                                       self.biases['encoder_b1'+ str(self.order)]))\n",
        "        layer_2 = tf.nn.tanh(tf.add(tf.matmul(layer_1, self.weights['encoder_h2'+ str(self.order)]),\n",
        "                                       self.biases['encoder_b2'+ str(self.order)]))\n",
        "        layer_3 = tf.reshape(tf.nn.tanh(tf.add(tf.matmul(layer_2, self.weights['encoder_h3'+ str(self.order)]),\n",
        "                                       self.biases['encoder_b3'+ str(self.order)])),[batch_size,1,self.output_size])\n",
        "        return layer_3\n",
        "\n",
        "\n",
        "    # CANE's version of conv()\n",
        "    def conv(self):\n",
        "        W2 = tf.Variable(tf.random.truncated_normal([2, embed_size // 2, 1, 100], stddev=0.3))\n",
        "        rand_matrix = tf.Variable(tf.random.truncated_normal([100, 100], stddev=0.3))\n",
        "\n",
        "        convA = tf.nn.conv2d(self.T_A, W2, strides=[1, 1, 1, 1], padding='VALID')\n",
        "        convB = tf.nn.conv2d(self.T_B, W2, strides=[1, 1, 1, 1], padding='VALID')\n",
        "        convPOS = tf.nn.conv2d(self.T_POS, W2, strides=[1, 1, 1, 1], padding='VALID')\n",
        "        convNEG = tf.nn.conv2d(self.T_NEG, W2, strides=[1, 1, 1, 1], padding='VALID')\n",
        "\n",
        "        hA = tf.tanh(tf.squeeze(convA))\n",
        "        hB = tf.tanh(tf.squeeze(convB))\n",
        "        hPOS = tf.tanh(tf.squeeze(convPOS))\n",
        "        hNEG = tf.tanh(tf.squeeze(convNEG))\n",
        "\n",
        "        tmphA = tf.reshape(hA, [batch_size * (MAX_LEN - 1), embed_size // 2])\n",
        "        ha_mul_rand = tf.reshape(tf.matmul(tmphA, rand_matrix), [batch_size, MAX_LEN - 1, embed_size // 2])\n",
        "\n",
        "        r1 = tf.matmul(ha_mul_rand, hB, adjoint_b=True)\n",
        "        r3 = tf.matmul(ha_mul_rand, hNEG, adjoint_b=True)\n",
        "        r4 = tf.matmul(ha_mul_rand, hPOS, adjoint_b=True)\n",
        "        att1 = tf.expand_dims(tf.stack(r1), -1)\n",
        "        att3 = tf.expand_dims(tf.stack(r3), -1)\n",
        "        att4 = tf.expand_dims(tf.stack(r4), -1)\n",
        "\n",
        "        att1 = tf.tanh(att1)\n",
        "        att3 = tf.tanh(att3)\n",
        "        att4 = tf.tanh(att4)\n",
        "\n",
        "\n",
        "        pooled_A = tf.reduce_mean(att1, 2)\n",
        "        pooled_B = tf.reduce_mean(att1, 1)\n",
        "        pooled_POS = tf.reduce_mean(att4, 1)\n",
        "        pooled_NEG = tf.reduce_mean(att3, 1)\n",
        "\n",
        "        a_flat = tf.squeeze(pooled_A)\n",
        "        b_flat = tf.squeeze(pooled_B)\n",
        "        pos_flat = tf.squeeze(pooled_POS)\n",
        "        neg_flat = tf.squeeze(pooled_NEG)\n",
        "\n",
        "        w_A = tf.nn.softmax(a_flat)\n",
        "        w_B = tf.nn.softmax(b_flat)\n",
        "        w_POS = tf.nn.softmax(pos_flat)\n",
        "        w_NEG = tf.nn.softmax(neg_flat)\n",
        "\n",
        "        rep_A = tf.expand_dims(w_A, -1)\n",
        "        rep_B = tf.expand_dims(w_B, -1)\n",
        "        rep_POS = tf.expand_dims(w_POS, -1)\n",
        "        rep_NEG = tf.expand_dims(w_NEG, -1)\n",
        "\n",
        "        hA = tf.transpose(hA, perm=[0, 2, 1])\n",
        "        hB = tf.transpose(hB, perm=[0, 2, 1])\n",
        "        hPOS = tf.transpose(hPOS, perm=[0, 2, 1])\n",
        "        hNEG = tf.transpose(hNEG, perm=[0, 2, 1])\n",
        "\n",
        "        rep1 = tf.matmul(hA, rep_A)\n",
        "        rep2 = tf.matmul(hB, rep_B)\n",
        "        rep3 = tf.matmul(hNEG, rep_NEG)\n",
        "        rep4 = tf.matmul(hPOS, rep_POS)\n",
        "\n",
        "        self.u_A = tf.squeeze(rep1)\n",
        "        self.u_B = tf.squeeze(rep2)\n",
        "        self.u_P = tf.squeeze(rep4)\n",
        "        self.u_N = tf.squeeze(rep3)\n",
        "\n",
        "        R_AB = self.u_A + self.u_B\n",
        "        self.R_AB = tf.reshape(R_AB, [batch_size, embed_size // 2])\n",
        "\n",
        "        l_predict = self.encoder(tf.reshape(self.R_AB, [batch_size, 1, embed_size // 2]))\n",
        "        return R_AB, l_predict\n",
        "\n",
        "    def compute_loss(self,order):\n",
        "\n",
        "        rho1 = 0.3\n",
        "        rho2 = 0.1\n",
        "        rho3 = 0.3\n",
        "        p1 = tf.reduce_sum(tf.multiply(self.u_A, self.u_B), 1)\n",
        "        p1 = tf.math.log(tf.nn.sigmoid(p1) + 0.001)\n",
        "        p2 = tf.reduce_sum(tf.multiply(self.N_A, self.N_B), 1)\n",
        "        p2 = tf.math.log(tf.nn.sigmoid(p2) + 0.001)\n",
        "\n",
        "        p3 = tf.reduce_sum(tf.multiply(self.N_A, self.u_A), 1)\n",
        "        p3 = tf.math.log(tf.nn.sigmoid(p3) + 0.001)\n",
        "        p4 = tf.math.reduce_sum(tf.multiply(self.N_B, self.u_B), 1)\n",
        "        p4 = tf.math.log(tf.nn.sigmoid(p4) + 0.001)\n",
        "\n",
        "\n",
        "        for i in range(0, 1):\n",
        "        # for i in range(0, config.negative_ratio):\n",
        "            u_P1 = tf.reshape(tf.split(self.u_P, negative_ratio, 1)[i], [batch_size, embed_size // 2])\n",
        "            u_N1 = tf.reshape(tf.split(self.u_N, negative_ratio, 1)[i],\n",
        "                              [batch_size, embed_size // 2])\n",
        "            p5 = tf.reduce_sum(tf.multiply(self.u_A, u_P1), 1)\n",
        "            p5 = tf.math.log(tf.nn.sigmoid(p5) + 0.001)\n",
        "            p6 = tf.reduce_sum(tf.multiply(self.u_B, u_P1), 1)\n",
        "            p6 = tf.math.log(tf.nn.sigmoid(p6) + 0.001)\n",
        "            p7 = tf.reduce_sum(tf.multiply(self.u_A, u_N1), 1)\n",
        "            p7 = tf.math.log(tf.nn.sigmoid(-p7) + 0.001)\n",
        "            p8 = tf.reduce_sum(tf.multiply(self.u_B, u_N1), 1)\n",
        "            p8 = tf.math.log(tf.nn.sigmoid(-p8) + 0.001)\n",
        "            p9 = tf.reduce_sum(tf.multiply(self.N_A, self.N_NEG), 1)\n",
        "            p9 = tf.math.log(tf.nn.sigmoid(-p9) + 0.001)\n",
        "            p10 = tf.reduce_sum(tf.multiply(self.N_B, self.N_NEG), 1)\n",
        "            p10 = tf.math.log(tf.nn.sigmoid(-p10) + 0.001)\n",
        "\n",
        "        p11 = tf.reduce_sum(tf.multiply(self.l_predict, self.relation))\n",
        "        p11 = tf.math.log(tf.nn.sigmoid(p11) + 0.001)\n",
        "\n",
        "        p_all = rho1*(p1 + p2 + p5 + p6 + p7 + p8 + p9 + p10) + rho2*(p3 + p4) + rho3 * p11\n",
        "\n",
        "        temp_loss = -tf.reduce_sum(p_all+p11)\n",
        "        self.sample_sum1 = tf.reduce_sum(tf.exp(tf.multiply(self.pos_ab, self.neg_ab)),\n",
        "                                         axis=1)\n",
        "        self.first_loss = tf.reduce_mean(-tf.reduce_sum(tf.multiply(self.N_A, self.N_B), axis=1) +\n",
        "                                         tf.math.log(self.sample_sum1))\n",
        "        self.sample_sum2 = tf.reduce_sum(\n",
        "            tf.exp(tf.multiply(self.pos_ab, self.neg_ab_context)), axis=1)\n",
        "        self.second_loss = tf.reduce_mean(-tf.reduce_sum(tf.multiply(self.N_A, self.pos_nb_context), axis=1) +\n",
        "                                          tf.math.log(self.sample_sum2))\n",
        "        loss = temp_loss + self.first_loss + self.second_loss\n",
        "\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "Zqqn_6nzwuUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABKadvVjB7St"
      },
      "source": [
        "# ***Negative table***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12O1kWaqB-0x"
      },
      "outputs": [],
      "source": [
        "def InitNegTable(edges):\n",
        "\ta_list,b_list=zip(*edges)\n",
        "\ta_list=list(a_list)\n",
        "\tb_list=list(b_list)\n",
        "\tnode=a_list\n",
        "\tnode.extend(b_list)\n",
        "\n",
        "\tnode_degree={}\n",
        "\tfor i in node:\n",
        "\t\tif i in node_degree:\n",
        "\t\t\tnode_degree[i]+=1\n",
        "\t\telse:\n",
        "\t\t\tnode_degree[i]=1\n",
        "\tsum_degree=0\n",
        "\tfor i in node_degree.values():\n",
        "\t\tsum_degree+=pow(i,0.75)\n",
        "\n",
        "\tpor=0\n",
        "\tcur_sum=0\n",
        "\tvid=-1\n",
        "\tneg_table=[]\n",
        "\tdegree_list=list(node_degree.values())\n",
        "\tnode_id=list(node_degree.keys())\n",
        "\tfor i in range(neg_table_size):\n",
        "\t\tif(((i+1)/float(neg_table_size))>por):\n",
        "\t\t\tcur_sum+=pow(degree_list[vid+1],NEG_SAMPLE_POWER)\n",
        "\t\t\tpor=cur_sum/sum_degree\n",
        "\t\t\tvid+=1\n",
        "\t\tneg_table.append(node_id[vid])\n",
        "\treturn neg_table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-y0bRdobCaxz"
      },
      "source": [
        "# ***Classify***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irxGxTo8CcpG"
      },
      "outputs": [],
      "source": [
        "class TopKRanker(OneVsRestClassifier):\n",
        "    def predict(self, X, top_k_list):\n",
        "        probs = np.asarray(super(TopKRanker, self).predict_proba(X))\n",
        "        all_labels = []\n",
        "        for i, k in enumerate(top_k_list):\n",
        "            probs_ = probs[i, :]\n",
        "            labels = self.classes_[probs_.argsort()[-k:]].tolist()\n",
        "            probs_[:] = 0\n",
        "            probs_[labels] = 1\n",
        "            all_labels.append(probs_)\n",
        "        return np.asarray(all_labels)\n",
        "\n",
        "\n",
        "class Classifier(object):\n",
        "\n",
        "    def __init__(self, vectors, clf):\n",
        "        self.embeddings = vectors\n",
        "        self.clf = TopKRanker(clf)\n",
        "        self.binarizer = MultiLabelBinarizer(sparse_output=True)\n",
        "\n",
        "    def train(self, X, Y, Y_all):\n",
        "        self.binarizer.fit(Y_all)\n",
        "        # X_train = [self.embeddings[x] for x in X]\n",
        "        X_train = [self.embeddings[int(x)] for x in X] # For each node in X, take its embedding\n",
        "        Y = self.binarizer.transform(Y)\n",
        "        self.clf.fit(X_train, Y)\n",
        "\n",
        "    def evaluate(self, X, Y):\n",
        "        top_k_list = [len(l) for l in Y] # For each label in Y, take its size (multi-label)\n",
        "        Y_ = self.predict(X, top_k_list)\n",
        "        Y = self.binarizer.transform(Y)\n",
        "        averages = [\"micro\", \"macro\"]\n",
        "        results = {}\n",
        "        for average in averages:\n",
        "            results[average] = f1_score(Y, Y_, average=average)\n",
        "        return results\n",
        "\n",
        "    def predict(self, X, top_k_list):\n",
        "        X_ = np.asarray([self.embeddings[int(x)] for x in X])\n",
        "        Y = self.clf.predict(X_, top_k_list=top_k_list)\n",
        "        return Y\n",
        "\n",
        "    def split_train_evaluate(self, X, Y, train_precent, seed=0):\n",
        "        state = np.random.get_state()\n",
        "        training_size = int(train_precent * len(X)) # Set the ratio based on the size of X\n",
        "        np.random.seed(seed)\n",
        "        shuffle_indices = np.random.permutation(np.arange(len(X))) # Shuffle the indices of X (X contains all nodes)\n",
        "\n",
        "        # Access the values of X and Y based on the shuffled indices\n",
        "\n",
        "        # X_train and Y_train will have \"training_size\" number of values of X and Y\n",
        "        X_train = [X[shuffle_indices[i]] for i in range(training_size)]\n",
        "        Y_train = [Y[shuffle_indices[i]] for i in range(training_size)]\n",
        "\n",
        "        # X_test and Y_test will have \"len(X) - training_size\" number of values of X and Y\n",
        "        X_test = [X[shuffle_indices[i]] for i in range(training_size, len(X))]\n",
        "        Y_test = [Y[shuffle_indices[i]] for i in range(training_size, len(X))]\n",
        "\n",
        "        self.train(X_train, Y_train, Y) # Y has the labels of all nodes\n",
        "        np.random.set_state(state)\n",
        "        return self.evaluate(X_test, Y_test)\n",
        "\n",
        "\n",
        "\n",
        "def load_embeddings(filename):\n",
        "    fin = open(filename, 'r')\n",
        "    node_num, size = [int(x) for x in fin.readline().strip().split()]\n",
        "    vectors = {}\n",
        "    while 1:\n",
        "        l = fin.readline()\n",
        "        if l == '':\n",
        "            break\n",
        "        vec = l.strip().split(' ')\n",
        "        assert len(vec) == size + 1\n",
        "        vectors[vec[0]] = [float(x) for x in vec[1:]]\n",
        "    fin.close()\n",
        "    assert len(vectors) == node_num\n",
        "    return vectors\n",
        "\n",
        "def read_node_label(filename):\n",
        "    fin = open(filename, 'r')\n",
        "    X = []\n",
        "    Y = []\n",
        "    XY_dic = {}\n",
        "    X_Y_dic = {}\n",
        "    while 1:\n",
        "        l = fin.readline()\n",
        "        if l == '':\n",
        "            break\n",
        "        # vec = l.strip().split('\\t')\n",
        "        vec = l.strip().split(' ')\n",
        "        X.append(vec[0])\n",
        "        Y.append(vec[1:])\n",
        "        X_Y_dic[str(vec[0])] = str(vec[1:][0])\n",
        "        XY_dic.setdefault(str(vec[1:][0]), []).append(str(vec[0]))\n",
        "    fin.close()\n",
        "    return X, Y, XY_dic, X_Y_dic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zOfZReFCOI1"
      },
      "source": [
        "# ***Train***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfXKB5YC3cCS"
      },
      "source": [
        "Code that needs to be executed only once"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VELZ1oY_3k-T"
      },
      "outputs": [],
      "source": [
        "def get_key(dict, value):\n",
        "    return [k for k, v in dict.items() if v == value]\n",
        "\n",
        "\n",
        "# Store the label of each abstract\n",
        "label_dic = {}\n",
        "with open(f'{parent_path}/{categories_file}', 'r') as f:\n",
        "  labels = f.readlines()\n",
        "\n",
        "\"\"\"\n",
        "The \"node_tag\" text file has to be in the following format: node_id node_label1 node_label2 ...\n",
        "\"\"\"\n",
        "for la in labels:\n",
        "  label_dic[la.split()[0]] = la.split()[1:][0] # la.split()[0] = the node id ----- la.split()[1:][0] = The label of that node. If a node has many labels, take the first\n",
        "\n",
        "zero_list = []\n",
        "\n",
        "# Place \"embed_size * 2\" (400) zeros in \"zero_list\"\n",
        "for i in range(0, embed_size):\n",
        "  zero_list.append(0)\n",
        "zero_list = np.array(zero_list) # My addition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXY6tbddHIaz"
      },
      "source": [
        "## ***Single Execution***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtL36FG8WNNQ"
      },
      "outputs": [],
      "source": [
        "# Save embeddings with a unique name\n",
        "#embed_file = f\"{parent_path}/Results/DeepEmLAN/embed_link_pred_{graph_file.split('.')[0]}_{data_text_file.split('.')[0]}.txt\"\n",
        "embed_file = f\"{parent_path}/Results/DeepEmLAN/embed_node_clf_{graph_file.split('.')[0]}_{data_text_file.split('.')[0]}.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1Wa1lNLCP5Z"
      },
      "outputs": [],
      "source": [
        "# Create the edge list. Store the unique nodes in the list \"nodes\"\n",
        "with open(f'{parent_path}/{graph_file}', 'r') as f:\n",
        "  eedges = f.readlines()\n",
        "\n",
        "edge_list = []\n",
        "nodes = [] # \"nodes\" will contain all the unique nodes of the graph\n",
        "\n",
        "for ee in eedges:\n",
        "  edge_list.append(list(ee.split()))\n",
        "\n",
        "for ll in edge_list:\n",
        "  for ed in ll:\n",
        "    if ed not in nodes:\n",
        "      nodes.append(ed)\n",
        "    else:\n",
        "      continue\n",
        "\n",
        "\n",
        "# Create the dataset. It seems that dataSet() takes the full graph. Perhaps because it is meant for node classification and not for link prediction\n",
        "# In link prediction, other NE methods split the graph and the dataSet() takes only a portion of the graph\n",
        "data = dataSet(f'{parent_path}/{data_text_file}', f'{parent_path}/{graph_file}', label_dic)\n",
        "\n",
        "\n",
        "# Train the model for \"num_epoch\" epochs\n",
        "model = Model(data, 1)\n",
        "\n",
        "\n",
        "start_time = datetime.now()\n",
        "for i in range(num_epoch):\n",
        "  model.train_one_epoch()\n",
        "\n",
        "vectors, available_vectors = model.get_embedding() # Get the node embeddings\n",
        "\n",
        "# Store all the node ids with the same label as node \"ii\".\n",
        "node_nei_list = {}\n",
        "one_node_edges = []\n",
        "for ii in nodes: # For each node \"ii\"\n",
        "  for ed in edge_list: # For each edge \"ed\"\n",
        "    if ii in ed: # If node \"ii\" is in \"ed\". This takes the one-hop neighborhood of \"ii\"\n",
        "      if label_dic[ii] == label_dic[ed[0]] and ii not in one_node_edges: # If the label of \"ii\" is the same as the label of \"ed[0]\" and \"ii\" not in \"one_node_edges\"\n",
        "        one_node_edges.append(ed[0])\n",
        "      if label_dic[ii] == label_dic[ed[1]] and ii not in one_node_edges:\n",
        "        one_node_edges.append(ed[1])\n",
        "    else:\n",
        "      pass\n",
        "  node_nei_list[ii] = one_node_edges # Node \"ii\" has the same label with nodes in \"one_node_edges\"\n",
        "  one_node_edges = []\n",
        "\n",
        "\n",
        "\n",
        "# Change the vector representation of each node\n",
        "new_vector = {}\n",
        "one_node_new_vec = []\n",
        "for ve in vectors.keys(): # For each node that has an embedding in \"vectors\"\n",
        "  if str(ve) in node_nei_list:\n",
        "    for nnl in node_nei_list[str(ve)]: # Take the nodes with the same label as node \"ve\"\n",
        "      one_node_new_vec.append(vectors[int(nnl)]) # Append the embeddings of all nodes with the same label as \"ve\"\n",
        "    #one_node_new_vec = np.array(one_node_new_vec).sum(axis=0)/len(node_nei_list[str(ve)]) # Take the mean\n",
        "\n",
        "    if one_node_new_vec: # Check if one_node_new_vec has elements before calculating the mean to avoid ZeroDivisionError\n",
        "      one_node_new_vec = np.array(one_node_new_vec).sum(axis=0)/len(node_nei_list[str(ve)]) # Take the mean\n",
        "    else: # Assign zero_list if the node has no neighbors with the same label\n",
        "      one_node_new_vec = zero_list\n",
        "\n",
        "    # The new vector for node \"ve\" will be the mean of all the embeddings of the one-hop neighbors with the same label as \"ve\"\n",
        "    new_vector[ve] = one_node_new_vec\n",
        "    one_node_new_vec = []\n",
        "  else:\n",
        "    new_vector[ve] = zero_list\n",
        "\n",
        "end_time = datetime.now()\n",
        "print(f'Time: {((end_time - start_time).total_seconds()) / 60.0}')\n",
        "model.close_session()\n",
        "\n",
        "\n",
        "# For link prediction\n",
        "''' with open(embed_file, 'wb') as f:\n",
        "  for node_id, node_vec in new_vector.items():\n",
        "    if available_vectors[node_id] == 1:\n",
        "        f.write((' '.join(map(str, node_vec)) + '\\n').encode())\n",
        "    else:\n",
        "        f.write('\\n'.encode()) '''\n",
        "\n",
        "# For node classification\n",
        "with open(embed_file, 'wb') as f:\n",
        "  for node_id, node_vec in new_vector.items():\n",
        "    f.write((' '.join(map(str, node_vec)) + '\\n').encode())\n",
        "\n",
        "gc.collect()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qjl3aVXJuZMh"
      },
      "source": [
        "### ***Link Prediction*** (For both single and multiple executions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9vCWo-WGvKf"
      },
      "outputs": [],
      "source": [
        "embed_files = [[f'{parent_path}/Results/DeepEmLAN/embed_link_pred_sgraph15_data-v3-500.txt',\n",
        "               f'{parent_path}/Results/DeepEmLAN/embed_link_pred_sgraph15_data-v3-500C.txt',\n",
        "               f'{parent_path}/Results/DeepEmLAN/embed_link_pred_sgraph15_YAKE10.txt',\n",
        "               f'{parent_path}/Results/DeepEmLAN/embed_link_pred_sgraph15_PosR10.txt',\n",
        "               f'{parent_path}/Results/DeepEmLAN/embed_link_pred_sgraph15_PosR5.txt',\n",
        "               f'{parent_path}/Results/DeepEmLAN/embed_link_pred_sgraph15_YAKE5.txt',\n",
        "               f'{parent_path}/Results/DeepEmLAN/embed_link_pred_sgraph15_RAKE10.txt',\n",
        "               f'{parent_path}/Results/DeepEmLAN/embed_link_pred_sgraph15_RAKE5.txt',\n",
        "               f'{parent_path}/Results/DeepEmLAN/embed_link_pred_sgraph15_RAKE10C.txt',\n",
        "               f'{parent_path}/Results/DeepEmLAN/embed_link_pred_sgraph15_RAKE5C.txt',\n",
        "               f'{parent_path}/Results/DeepEmLAN/embed_link_pred_sgraph15_TFIDF5.txt',\n",
        "               f'{parent_path}/Results/DeepEmLAN/embed_link_pred_sgraph15_TFIDF10.txt',\n",
        "               f'{parent_path}/Results/DeepEmLAN/embed_link_pred_sgraph15_TextR10.txt',\n",
        "               f'{parent_path}/Results/DeepEmLAN/embed_link_pred_sgraph15_TextR5.txt',\n",
        "               f'{parent_path}/Results/DeepEmLAN/embed_link_pred_sgraph15_TopicR10.txt',\n",
        "               f'{parent_path}/Results/DeepEmLAN/embed_link_pred_sgraph15_TopicR5.txt'],\n",
        "\n",
        "               [f'{parent_path}/Results/DeepEmLAN/embed_link_pred_sgraph45_data-v3-500.txt',\n",
        "               f'{parent_path}/Results/DeepEmLAN/embed_link_pred_sgraph45_data-v3-500C.txt',\n",
        "               f'{parent_path}/Results/DeepEmLAN/embed_link_pred_sgraph45_YAKE10.txt',\n",
        "               f'{parent_path}/Results/DeepEmLAN/embed_link_pred_sgraph45_PosR10.txt',\n",
        "               f'{parent_path}/Results/DeepEmLAN/embed_link_pred_sgraph45_PosR5.txt',\n",
        "               f'{parent_path}/Results/DeepEmLAN/embed_link_pred_sgraph45_YAKE5.txt',\n",
        "               f'{parent_path}/Results/DeepEmLAN/embed_link_pred_sgraph45_RAKE10.txt',\n",
        "               f'{parent_path}/Results/DeepEmLAN/embed_link_pred_sgraph45_RAKE5.txt',\n",
        "               f'{parent_path}/Results/DeepEmLAN/embed_link_pred_sgraph45_RAKE10C.txt',\n",
        "               f'{parent_path}/Results/DeepEmLAN/embed_link_pred_sgraph45_RAKE5C.txt',\n",
        "               f'{parent_path}/Results/DeepEmLAN/embed_link_pred_sgraph45_TFIDF5.txt',\n",
        "               f'{parent_path}/Results/DeepEmLAN/embed_link_pred_sgraph45_TFIDF10.txt',\n",
        "               f'{parent_path}/Results/DeepEmLAN/embed_link_pred_sgraph45_TextR10.txt',\n",
        "               f'{parent_path}/Results/DeepEmLAN/embed_link_pred_sgraph45_TextR5.txt',\n",
        "               f'{parent_path}/Results/DeepEmLAN/embed_link_pred_sgraph45_TopicR10.txt',\n",
        "               f'{parent_path}/Results/DeepEmLAN/embed_link_pred_sgraph45_TopicR5.txt'],\n",
        "\n",
        "               [f'{parent_path}/Results/DeepEmLAN/embed_link_pred_sgraph75_data-v3-500.txt',\n",
        "               f'{parent_path}/Results/DeepEmLAN/embed_link_pred_sgraph75_data-v3-500C.txt',\n",
        "               f'{parent_path}/Results/DeepEmLAN/embed_link_pred_sgraph75_YAKE10.txt',\n",
        "               f'{parent_path}/Results/DeepEmLAN/embed_link_pred_sgraph75_PosR10.txt',\n",
        "               f'{parent_path}/Results/DeepEmLAN/embed_link_pred_sgraph75_PosR5.txt',\n",
        "               f'{parent_path}/Results/DeepEmLAN/embed_link_pred_sgraph75_YAKE5.txt',\n",
        "               f'{parent_path}/Results/DeepEmLAN/embed_link_pred_sgraph75_RAKE10.txt',\n",
        "               f'{parent_path}/Results/DeepEmLAN/embed_link_pred_sgraph75_RAKE5.txt',\n",
        "               f'{parent_path}/Results/DeepEmLAN/embed_link_pred_sgraph75_RAKE10C.txt',\n",
        "               f'{parent_path}/Results/DeepEmLAN/embed_link_pred_sgraph75_RAKE5C.txt',\n",
        "               f'{parent_path}/Results/DeepEmLAN/embed_link_pred_sgraph75_TFIDF5.txt',\n",
        "               f'{parent_path}/Results/DeepEmLAN/embed_link_pred_sgraph75_TFIDF10.txt',\n",
        "               f'{parent_path}/Results/DeepEmLAN/embed_link_pred_sgraph75_TextR10.txt',\n",
        "               f'{parent_path}/Results/DeepEmLAN/embed_link_pred_sgraph75_TextR5.txt',\n",
        "               f'{parent_path}/Results/DeepEmLAN/embed_link_pred_sgraph75_TopicR10.txt',\n",
        "               f'{parent_path}/Results/DeepEmLAN/embed_link_pred_sgraph75_TopicR5.txt']]\n",
        "\n",
        "\n",
        "# Initialize a log file to store the AUC results\n",
        "with open(f'{parent_path}/Results/DeepEmLAN/{link_pred_results_file}', \"a\") as f:\n",
        "    f.write(\"Embed File\\tAUC Value\\n\")\n",
        "\n",
        "for tgfi, tgf in enumerate(test_graph_files):\n",
        "  for ef in embed_files[tgfi]:\n",
        "      node2vec = {}\n",
        "\n",
        "      # Load the embeddings from the current embed file\n",
        "      with open(ef, 'rb') as f:\n",
        "          for i, j in enumerate(f):\n",
        "              if j.decode() != '\\n':\n",
        "                  node2vec[i] = list(map(float, j.strip().decode().split()))\n",
        "\n",
        "      # Load the edges from the test graph file\n",
        "      with open(f'{parent_path}/{tgf}', 'rb') as f:\n",
        "          edges = [list(map(int, i.strip().decode().split())) for i in f]\n",
        "\n",
        "      nodes = list(set([i for j in edges for i in j])) # All the unique nodes in \"edges\"\n",
        "\n",
        "      # Calculate AUC\n",
        "      a = 0\n",
        "      b = 0\n",
        "      for i, j in edges:\n",
        "          if i in node2vec.keys() and j in node2vec.keys():\n",
        "              dot1 = np.dot(node2vec[i], node2vec[j])\n",
        "              random_node = random.sample(nodes, 1)[0]\n",
        "              while random_node == j or random_node not in node2vec.keys():\n",
        "                  random_node = random.sample(nodes, 1)[0]\n",
        "              dot2 = np.dot(node2vec[i], node2vec[random_node])\n",
        "              if dot1 > dot2:\n",
        "                  a += 1\n",
        "              elif dot1 == dot2:\n",
        "                  a += 0.5\n",
        "              b += 1\n",
        "\n",
        "      auc_value = float(a) / b if b > 0 else 0\n",
        "      print(f\"AUC value for {ef.split('/')[-1]}: {auc_value}\")\n",
        "\n",
        "      # Log the result\n",
        "      with open(f'{parent_path}/Results/DeepEmLAN/{link_pred_results_file}', \"a\") as f:\n",
        "          f.write(f\"{ef}\\t{tgf}\\t{auc_value}\\n\")\n",
        "\n",
        "      gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LivQHT84uc-h"
      },
      "source": [
        "### ***Node Classification*** (For both single and multiple executions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHXB5pVjGMab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "feaf49d0-f97c-40e5-c95c-c027b53bacc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "embed_node_clf_graph_data.txt\n",
            "{'0.15-micro': 0.8866374589266156}\n",
            "{'0.15-macro': 0.8678865872879209}\n",
            "\n",
            "{'0.45-micro': 0.9069373942470389}\n",
            "{'0.45-macro': 0.8939780853888418}\n",
            "\n",
            "{'0.75-micro': 0.9143389199255122}\n",
            "{'0.75-macro': 0.9038831443944323}\n",
            "\n",
            "embed_node_clf_graph_YAKE.txt\n",
            "{'0.15-micro': 0.946330777656079}\n",
            "{'0.15-macro': 0.9414316831961027}\n",
            "\n",
            "{'0.45-micro': 0.9551607445008459}\n",
            "{'0.45-macro': 0.9522637307048181}\n",
            "\n",
            "{'0.75-micro': 0.9497206703910613}\n",
            "{'0.75-macro': 0.9486867018336458}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "embed_files = [f'{parent_path}/Results/DeepEmLAN/embed_node_clf_graph_data-v3-500.txt',\n",
        "               f'{parent_path}/Results/DeepEmLAN/embed_node_clf_graph_data-v3-500C.txt',\n",
        "               f'{parent_path}/Results/DeepEmLAN/embed_node_clf_graph_YAKE10.txt',\n",
        "               f'{parent_path}/Results/DeepEmLAN/embed_node_clf_graph_PosR10.txt',\n",
        "               f'{parent_path}/Results/DeepEmLAN/embed_node_clf_graph_PosR5.txt',\n",
        "               f'{parent_path}/Results/DeepEmLAN/embed_node_clf_graph_YAKE5.txt',\n",
        "               f'{parent_path}/Results/DeepEmLAN/embed_node_clf_graph_RAKE10.txt',\n",
        "               f'{parent_path}/Results/DeepEmLAN/embed_node_clf_graph_RAKE5.txt',\n",
        "               f'{parent_path}/Results/DeepEmLAN/embed_node_clf_graph_RAKE10C.txt',\n",
        "               f'{parent_path}/Results/DeepEmLAN/embed_node_clf_graph_RAKE5C.txt',\n",
        "               f'{parent_path}/Results/DeepEmLAN/embed_node_clf_graph_TFIDF5.txt',\n",
        "               f'{parent_path}/Results/DeepEmLAN/embed_node_clf_graph_TFIDF10.txt',\n",
        "               f'{parent_path}/Results/DeepEmLAN/embed_node_clf_graph_TextR10.txt',\n",
        "               f'{parent_path}/Results/DeepEmLAN/embed_node_clf_graph_TextR5.txt',\n",
        "               f'{parent_path}/Results/DeepEmLAN/embed_node_clf_graph_TopicR10.txt',\n",
        "               f'{parent_path}/Results/DeepEmLAN/embed_node_clf_graph_TopicR5.txt']\n",
        "\n",
        "\n",
        "with open(f'{parent_path}/{categories_file}', 'r') as f:\n",
        "  tags = f.readlines() # \"tags\" will be a 2D list. Each sublist will have the form: nodeID     label\n",
        "\n",
        "if train_classifier:\n",
        "\n",
        "  clf_test_len = len(nodes) # The number of nodes will be the same in each run since we're using the whole graph and thus, all of its nodes\n",
        "\n",
        "  for ef in embed_files:\n",
        "    X = []\n",
        "    Y = []\n",
        "    new_vector = get_vectors_from_file(ef)\n",
        "\n",
        "    for jk in range(0, clf_test_len):\n",
        "      if str(jk) in nodes: # If the index \"jk\" is a node\n",
        "        tag_list = tags[jk].strip().split() # For node \"jk\", take this info: jk     label\n",
        "        # Y.append([(int)(i) for i in tags])\n",
        "        lli = [str(i) for i in tag_list] # For node \"jk\", lli will contain all of its labels\n",
        "        if len(lli) != 0:\n",
        "          if np.array(new_vector[jk]).any() != np.array(zero_list).any(): # If there is no zero value in the embedding of \"jk\"\n",
        "            X.append(jk)\n",
        "            Y.append(lli[1:][0]) # Take the first label (if there are multiple) of node \"jk\"\n",
        "\n",
        "    # This part of the code uses only the X and Y lists created above\n",
        "    mi = {}\n",
        "    ma = {}\n",
        "    li1 = []\n",
        "    li2 = []\n",
        "    #with open(f'{parent_path}/Results/DeepEmLAN/{node_clf_results_file}', 'a') as f:\n",
        "\n",
        "    #f.write(f\"{ef.split('/')[-1]} \\n\")\n",
        "    print(ef.split('/')[-1])\n",
        "\n",
        "    for i in range(0, len(clf_ratio)): # Experiment with each ratio\n",
        "      for j in range(0, clf_num): # clf_num = 5\n",
        "\n",
        "        clf = Classifier(vectors=new_vector, # All node embeddings\n",
        "                        clf=LogisticRegression())\n",
        "\n",
        "        result = clf.split_train_evaluate(X, Y, clf_ratio[i])\n",
        "\n",
        "        # Results\n",
        "        li1.append(result['micro'])\n",
        "        li2.append(result['macro'])\n",
        "\n",
        "      mi[str(str(clf_ratio[i]) + '-micro')] = sum(li1) / clf_num\n",
        "      ma[str(str(clf_ratio[i]) + '-macro')] = sum(li2) / clf_num\n",
        "\n",
        "      print(mi)\n",
        "      print(ma)\n",
        "      print()\n",
        "\n",
        "      #f.writelines(str(str(mi)+str(ma)))\n",
        "      #f.write('\\n')\n",
        "\n",
        "      # Reinitialize the dictionaries and lists\n",
        "      mi = {}\n",
        "      ma = {}\n",
        "      li1 = []\n",
        "      li2 = []\n",
        "\n",
        "    gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ys29692wH8_t"
      },
      "source": [
        "## ***Multiple Executions***"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Version-1 (Original)"
      ],
      "metadata": {
        "id": "KkLTGNh4y6vg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2G6xeAI_H_3I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "202eefcd-8751-45f0-c1d1-d7197bebcb3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The maximum length is: 411\n",
            "Total load 5214 edges.\n",
            "Processing graph: graph.txt, text: data.txt\n",
            "Time: 2.088106983333333 min\n",
            "The maximum length is: 16\n",
            "Total load 5214 edges.\n",
            "Processing graph: graph.txt, text: YAKE.txt\n",
            "Time: 0.5837468833333334 min\n"
          ]
        }
      ],
      "source": [
        "for gf in ['graph.txt']: #for gf in split_graph_files: # For link prediction. For node classification just use: for gf in ['graph.txt']:\n",
        "\n",
        "  # Create the edge list. Store the unique nodes in the list \"nodes\"\n",
        "  with open(f'{parent_path}/{gf}', 'r') as f:\n",
        "    eedges = f.readlines()\n",
        "\n",
        "  edge_list = []\n",
        "  nodes = []\n",
        "\n",
        "  for ee in eedges:\n",
        "    edge_list.append(list(ee.split()))\n",
        "\n",
        "  for ll in edge_list:\n",
        "    for ed in ll:\n",
        "      if ed not in nodes:\n",
        "        nodes.append(ed)\n",
        "      else:\n",
        "        continue\n",
        "\n",
        "  for t, txtf in enumerate(['data.txt', 'YAKE.txt']):\n",
        "\n",
        "    MAX_LEN = MAX_LENS[t]\n",
        "    print(f'The maximum length is: {MAX_LEN}')\n",
        "\n",
        "    # Create the dataset. It seems that dataSet() takes the full graph. Perhaps because it is meant for node classification and not for link prediction.\n",
        "    # In link prediction, other NE methods split the graph and the dataSet() takes only a portion of the graph\n",
        "    data = dataSet(f'{parent_path}/{txtf}', f'{parent_path}/{gf}', label_dic)\n",
        "\n",
        "    # Logging the execution details\n",
        "    #with open(f'{parent_path}/Results/DeepEmLAN/{log_file}', 'a') as f:\n",
        "    #    f.write(f'Processing graph: {gf}, text: {txtf}\\n')\n",
        "\n",
        "    print(f'Processing graph: {gf}, text: {txtf}')\n",
        "\n",
        "    # Train the model for \"num_epoch\" epochs\n",
        "    model = Model(data, 1)\n",
        "\n",
        "    start_time = datetime.now()\n",
        "    for i in range(num_epoch):\n",
        "      model.train_one_epoch()\n",
        "    end_time = datetime.now()\n",
        "\n",
        "    vectors, available_vectors = model.get_embedding() # Get the node embeddings\n",
        "\n",
        "\n",
        "    # Store all the node ids with the same label as node \"ii\".\n",
        "    node_nei_list = {}\n",
        "    one_node_edges = []\n",
        "    for ii in nodes: # For each node \"ii\"\n",
        "      for ed in edge_list: # For each edge \"ed\"\n",
        "        if ii in ed: # If node \"ii\" is in \"ed\". This takes the one-hop neighborhood of \"ii\"\n",
        "          if label_dic[ii] == label_dic[ed[0]] and ii not in one_node_edges: # If the label of \"ii\" is the same as the label of \"ed[0]\" and \"ii\" not in \"one_node_edges\"\n",
        "            one_node_edges.append(ed[0])\n",
        "          if label_dic[ii] == label_dic[ed[1]] and ii not in one_node_edges:\n",
        "            one_node_edges.append(ed[1])\n",
        "        else:\n",
        "          pass\n",
        "      node_nei_list[ii] = one_node_edges # Node \"ii\" has the same label with nodes in \"one_node_edges\"\n",
        "      one_node_edges = []\n",
        "\n",
        "\n",
        "    # Change the vector representation of each node\n",
        "    new_vector = {}\n",
        "    one_node_new_vec = []\n",
        "    for ve in vectors.keys(): # For each node that has an embedding in \"vectors\"\n",
        "      if str(ve) in node_nei_list:\n",
        "        for nnl in node_nei_list[str(ve)]: # Take the nodes with the same label as node \"ve\"\n",
        "          one_node_new_vec.append(vectors[int(nnl)]) # Append the embeddings of all nodes with the same label as \"ve\"\n",
        "        #one_node_new_vec = np.array(one_node_new_vec).sum(axis=0)/len(node_nei_list[str(ve)]) # Take the mean\n",
        "\n",
        "        if one_node_new_vec: # Check if one_node_new_vec has elements before calculating the mean to avoid ZeroDivisionError\n",
        "          one_node_new_vec = np.array(one_node_new_vec).sum(axis=0)/len(node_nei_list[str(ve)]) # Take the mean\n",
        "        else: # Assign zero_list if the node has no neighbors with the same label\n",
        "          one_node_new_vec = zero_list\n",
        "\n",
        "        # The new vector for node \"ve\" will be the mean of all the embeddings of the one-hop neighbors with the same label as \"ve\"\n",
        "        new_vector[ve] = one_node_new_vec\n",
        "        one_node_new_vec = []\n",
        "      else:\n",
        "        new_vector[ve] = zero_list\n",
        "\n",
        "    print(f'Time: {((end_time - start_time).total_seconds()) / 60.0} min')\n",
        "    model.close_session()\n",
        "\n",
        "    #with open(f'{parent_path}/Results/DeepEmLAN/{log_file}', 'a') as f:\n",
        "    #  f.write(f'Time: {((end_time - start_time).total_seconds()) / 60.0} min\\n')\n",
        "\n",
        "    # Save embeddings with a unique name\n",
        "    #embed_file = f\"{parent_path}/Results/DeepEmLAN/embed_link_pred_{gf.split('.')[0]}_{txtf.split('.')[0]}.txt\"\n",
        "    #embed_file = f\"{parent_path}/Results/DeepEmLAN/embed_node_clf_{gf.split('.')[0]}_{txtf.split('.')[0]}.txt\"\n",
        "    embed_file = f\"{parent_path}/embed_node_clf_{gf.split('.')[0]}_{txtf.split('.')[0]}.txt\"\n",
        "\n",
        "    ''' with open(embed_file, 'wb') as f:\n",
        "      for node_id, node_vec in new_vector.items():\n",
        "        if available_vectors[node_id] == 1:\n",
        "            f.write((' '.join(map(str, node_vec)) + '\\n').encode())\n",
        "        else:\n",
        "            f.write('\\n'.encode()) '''\n",
        "\n",
        "    with open(embed_file, 'wb') as f:\n",
        "      for node_id, node_vec in new_vector.items():\n",
        "        f.write((' '.join(map(str, node_vec)) + '\\n').encode())\n",
        "\n",
        "    # Log completion\n",
        "    #with open(f'{parent_path}/Results/DeepEmLAN/{log_file}', 'a') as f:\n",
        "    #    f.write(f'Embeddings saved to: {embed_file}\\n')\n",
        "\n",
        "    gc.collect()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Version-2"
      ],
      "metadata": {
        "id": "dLg9Iu4Sy9cA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for gf in ['graph.txt']:\n",
        "\n",
        "  with open(f'{parent_path}/{gf}', 'r') as f:\n",
        "    eedges = f.readlines()\n",
        "\n",
        "  edge_list = []\n",
        "  nodes = []\n",
        "\n",
        "  for ee in eedges:\n",
        "    edge_list.append(list(ee.split()))\n",
        "\n",
        "  for ll in edge_list:\n",
        "    for ed in ll:\n",
        "      if ed not in nodes:\n",
        "        nodes.append(ed)\n",
        "      else:\n",
        "        continue\n",
        "\n",
        "  for t, txtf in enumerate(['data.txt', 'YAKE.txt']):\n",
        "\n",
        "    MAX_LEN = MAX_LENS[t]\n",
        "    print(f'The maximum length is: {MAX_LEN}')\n",
        "\n",
        "    data = dataSet(f'{parent_path}/{txtf}', f'{parent_path}/{gf}', label_dic, text_filename=txtf)\n",
        "\n",
        "    #with open(f'{parent_path}/Results/DeepEmLAN/{log_file}', 'a') as f:\n",
        "    #    f.write(f'Processing graph: {gf}, text: {txtf}\\n')\n",
        "\n",
        "    print(f'Processing graph: {gf}, text: {txtf}')\n",
        "\n",
        "    with tf.Graph().as_default():\n",
        "      sess = tf.compat.v1.Session()\n",
        "      with sess.as_default():\n",
        "          model = Model(data, 1)\n",
        "          opt = tf.compat.v1.train.AdamOptimizer(lr)\n",
        "          train_op = opt.minimize(model.loss)\n",
        "          sess.run(tf.compat.v1.global_variables_initializer())\n",
        "\n",
        "          # Training\n",
        "          start_time = datetime.now()\n",
        "          for epoch in range(num_epoch):\n",
        "            loss_epoch = 0\n",
        "            batches_edges, batches_re, batches_neg, batches_br = data.generate_batches()\n",
        "            num_batch = len(batches_edges)\n",
        "            for i in range(num_batch):\n",
        "                node1, node2 = zip(*batches_edges[i])\n",
        "                node_list4 = batches_neg[i]\n",
        "                batch_r = batches_re[i]\n",
        "                batch_br = batches_br[i]\n",
        "                node_list3 = node2\n",
        "                node1, node2, node_list3, node_list4 = np.array(node1), np.array(node2), \\\n",
        "                                                              np.array(node_list3), np.array(node_list4)\n",
        "                node_list3 = np.transpose(node_list3)\n",
        "                text1, text2 = data.text[node1], data.text[node2]\n",
        "                text_pos = []\n",
        "                text_neg = []\n",
        "                for npp in node_list3:\n",
        "                    text_pos.append(data.text[npp])\n",
        "                for nn in node_list4:\n",
        "                    text_neg.append(data.text[nn])\n",
        "\n",
        "                feed_dict = {\n",
        "                    model.Text_a: text1,\n",
        "                    model.Text_b: text2,\n",
        "                    model.Text_pos: text_pos,\n",
        "                    model.Text_neg: text_neg,\n",
        "                    model.Node_a: node1,\n",
        "                    model.Node_b: node2,\n",
        "                    model.Node_pa: node_list3,\n",
        "                    model.Node_na: node_list4,\n",
        "                    model.relation: batch_r,\n",
        "                    model.brelation: batch_br\n",
        "\n",
        "                }\n",
        "\n",
        "                # run the graph\n",
        "                _, loss_batch = sess.run([train_op, model.loss], feed_dict=feed_dict)\n",
        "                # print loss_batch\n",
        "                loss_epoch += loss_batch\n",
        "\n",
        "          end_time = datetime.now()\n",
        "          #with open(f'{parent_path}/Results/DeepEmLAN/{log_file}', 'a') as f:\n",
        "          #    f.write(f'Time: {((end_time - start_time).total_seconds()) / 60.0} min\\n')\n",
        "\n",
        "          print(f'Total Time: {((end_time - start_time).total_seconds()) / 60.0} min')\n",
        "\n",
        "          # Save embeddings with a unique name\n",
        "          #embed_file = f\"{parent_path}/Results/DeepEmLAN/embed_link_pred_{gf.split('.')[0]}_{txtf.split('.')[0]}.txt\"\n",
        "          #embed_file = f\"{parent_path}/Results/DeepEmLAN/embed_node_clf_{gf.split('.')[0]}_{txtf.split('.')[0]}.txt\"\n",
        "          embed_file = f\"{parent_path}/embed_node_clf_{gf.split('.')[0]}_{txtf.split('.')[0]}.txt\"\n",
        "\n",
        "          vectors = {}\n",
        "          available_vectors = {}\n",
        "\n",
        "          embed = [[] for _ in range(data.num_nodes)]\n",
        "\n",
        "          batches_edges, batches_re, batches_neg, batches_br = data.generate_batches(mode='add')\n",
        "          num_batch = len(batches_edges)\n",
        "          for i in range(num_batch):\n",
        "              node1, node2 = zip(*batches_edges[i])\n",
        "              node_list4 = batches_neg[i]\n",
        "              batch_r = batches_re[i]\n",
        "              batch_br = batches_br[i]\n",
        "\n",
        "              node_list3 = node2\n",
        "              node1, node2, node_list3, node_list4 = np.array(node1), np.array(node2), \\\n",
        "                                                                  np.array(node_list3), np.array(node_list4)\n",
        "              node_list3 = np.transpose(node_list3)\n",
        "              text1, text2 = data.text[node1], data.text[node2]\n",
        "              text_pos = []\n",
        "              text_neg = []\n",
        "              for npp in node_list3:\n",
        "                  text_pos.append(data.text[npp])\n",
        "              for nn in node_list4:\n",
        "                  text_neg.append(data.text[nn])\n",
        "\n",
        "\n",
        "              feed_dict = {\n",
        "                  model.Text_a: text1,\n",
        "                  model.Text_b: text2,\n",
        "                  model.Text_pos: text_pos,\n",
        "                  model.Text_neg: text_neg,\n",
        "                  model.Node_a: node1,\n",
        "                  model.Node_b: node2,\n",
        "                  model.Node_pa: node_list3,\n",
        "                  model.Node_na: node_list4,\n",
        "                  model.relation: batch_r,\n",
        "                  model.brelation: batch_br\n",
        "\n",
        "              }\n",
        "              uA, uB, rAB, NA, NB = sess.run([model.u_A, model.u_B, model.R_AB, model.N_A, model.N_B], feed_dict=feed_dict)\n",
        "\n",
        "              for j in range(batch_size):\n",
        "                  embed[node1[j]].append(list(NA[j])+list(rAB[j]))  #\n",
        "                  embed[node2[j]].append(list(NB[j])+list(rAB[j]))  #\n",
        "\n",
        "          for i in range(data.num_nodes):\n",
        "              if embed[i]:\n",
        "                  tmp=np.sum(embed[i],axis=0)/len(embed[i])\n",
        "                  vectors[i]=tmp\n",
        "                  available_vectors[i] = 1\n",
        "              else:\n",
        "                  vectors[i]=zero_list\n",
        "                  available_vectors[i] = 0\n",
        "\n",
        "          node_nei_list = {}\n",
        "          one_node_edges = []\n",
        "          for ii in nodes:\n",
        "            for ed in edge_list:\n",
        "              if ii in ed:\n",
        "                if label_dic[ii] == label_dic[ed[0]] and ii not in one_node_edges:\n",
        "                  one_node_edges.append(ed[0])\n",
        "                if label_dic[ii] == label_dic[ed[1]] and ii not in one_node_edges:\n",
        "                  one_node_edges.append(ed[1])\n",
        "              else:\n",
        "                pass\n",
        "            node_nei_list[ii] = one_node_edges\n",
        "            one_node_edges = []\n",
        "\n",
        "          new_vector = {}\n",
        "          one_node_new_vec = []\n",
        "          for ve in vectors.keys():\n",
        "            if str(ve) in node_nei_list:\n",
        "              for nnl in node_nei_list[str(ve)]:\n",
        "                one_node_new_vec.append(vectors[int(nnl)])\n",
        "\n",
        "              if one_node_new_vec:\n",
        "                one_node_new_vec = np.array(one_node_new_vec).sum(axis=0)/len(node_nei_list[str(ve)])\n",
        "              else:\n",
        "                one_node_new_vec = zero_list\n",
        "\n",
        "              new_vector[ve] = one_node_new_vec\n",
        "              one_node_new_vec = []\n",
        "            else:\n",
        "              new_vector[ve] = zero_list\n",
        "\n",
        "          ''' with open(embed_file, 'wb') as f:\n",
        "            for node_id, node_vec in new_vector.items():\n",
        "              if available_vectors[node_id] == 1:\n",
        "                  f.write((' '.join(map(str, node_vec)) + '\\n').encode())\n",
        "              else:\n",
        "                  f.write('\\n'.encode()) '''\n",
        "\n",
        "          with open(embed_file, 'wb') as f:\n",
        "            for node_id, node_vec in new_vector.items():\n",
        "              f.write((' '.join(map(str, node_vec)) + '\\n').encode())\n",
        "\n",
        "          #with open(f'{parent_path}/Results/DeepEmLAN/{log_file}', 'a') as f:\n",
        "          #    f.write(f'Embeddings saved to: {embed_file}\\n')\n",
        "\n",
        "    gc.collect()"
      ],
      "metadata": {
        "id": "TDjXWt-txhlU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bea5edf4-7e86-4c1d-ec72-39158bd66ed9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The maximum length is: 411\n",
            "Total load 5214 edges.\n",
            "Processing graph: graph.txt, text: data.txt\n",
            "Total Time: 2.1275283166666665 min\n",
            "The maximum length is: 16\n",
            "Total load 5214 edges.\n",
            "Processing graph: graph.txt, text: YAKE.txt\n",
            "Total Time: 0.5802529 min\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCSSGchYI_iP"
      },
      "source": [
        "***IMPORTANT*** ==> Because the maximum length is always the same (100), the computation time, given different data text files, will always be approximately the same.\n",
        "\n",
        "\n",
        "***Hypothesis*** ==> When using the entire abstract, which consists of more that 100(MAX_LEN) words, we see a reduction in performance when compared to the keywords/keyphrases. This happens because the maximum words in all lines of the keyword files are less than 100 so we don't lose any information whereas the maximum words in all lines of the original data file (data.txt) exceed 100. Even if there are abstracts in data.txt with less than 100 words, they must be very few."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "ABKadvVjB7St",
        "-y0bRdobCaxz",
        "Qjl3aVXJuZMh",
        "dLg9Iu4Sy9cA"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}